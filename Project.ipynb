{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v_deA2IhVLVr"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGg_gEPTVLVu"
      },
      "source": [
        "# Part 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EYpW97VLVLVv",
        "outputId": "06e492d5-8e52-4ebf-a948-0f99b1b2d381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training..\n",
            "Evaluating..\n",
            "s output..\n",
            "Output successfully written!\n",
            "Training..\n",
            "Evaluating..\n",
            "s output..\n",
            "Output successfully written!\n",
            "Done for all datasets!!\n"
          ]
        }
      ],
      "source": [
        "#Part 1\n",
        "def train(filepath):\n",
        "\tprint(\"Training..\")\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\n",
        "\t# Set of all unique tokens in file\n",
        "\ttokens = []\n",
        "\t# Nested dictionary to keep track of emission count\n",
        "\t# {tag: {token: count} }\n",
        "\temission_count = {} \n",
        "\n",
        "\t# Iterate through file to update tokens and emission_count\n",
        "\tfor line in lines:\n",
        "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
        "\t\tif len(line_split) == 2:\n",
        "\t\t\ttoken = line_split[0]\n",
        "\t\t\ttag = line_split[1]\n",
        "\n",
        "\t\t\tif token not in tokens:\n",
        "\t\t\t\ttokens.append(token)\n",
        "\n",
        "\t\t\tif tag not in emission_count:\n",
        "\t\t\t\tnested_tag_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
        "\t\t\tif token not in nested_tag_dict:\n",
        "\t\t\t\tnested_tag_dict[token] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict[token] += 1\n",
        "\t\t\temission_count[tag] = nested_tag_dict\n",
        "\n",
        "\treturn tokens, emission_count\n",
        "\n",
        "\n",
        "def est_emission_param(emission_count, token, tag):\n",
        "\ttag_dict = emission_count[tag]\n",
        "\n",
        "\ta = tag_dict.get(token, 0)\t# Returns 0 if none\n",
        "\tb = sum(tag_dict.values())\n",
        "\n",
        "\treturn a / b\n",
        "\n",
        "\n",
        "def est_emission_param(emission_count, token, tag, k=1):\n",
        "\ttag_dict = emission_count[tag]\n",
        "\n",
        "\tif token != \"#UNK#\":\n",
        "\t\ta = tag_dict.get(token, 0)\n",
        "\telse:\n",
        "\t\ta = k \n",
        "\tb = sum(tag_dict.values()) + k\n",
        "\n",
        "\treturn a / b\n",
        "\n",
        "\n",
        "def get_sentence_tag(sentence, tokens, emission_count, k=1):\n",
        "\tpred_tags = []\n",
        "\n",
        "\tfor word in sentence:\n",
        "\t\tpred_tag = \"\"\n",
        "\t\tmax_emission = float('-inf')\n",
        "\n",
        "\t\tfor tag in emission_count:\n",
        "\t\t\tif word not in tokens:\n",
        "\t\t\t\tword = \"#UNK#\"\n",
        "\n",
        "\t\t\tif word in emission_count[tag] or word == \"#UNK#\":\n",
        "\t\t\t\temission = est_emission_param(emission_count, word, tag, k)\n",
        "\t\t\t\tif emission > max_emission:\n",
        "\t\t\t\t\tpred_tag = tag \n",
        "\t\t\t\t\tmax_emission = emission\n",
        "\n",
        "\t\tpred_tags.append(pred_tag)\n",
        "\n",
        "\treturn pred_tags\n",
        "\n",
        "\n",
        "def evaluate(filepath, tokens, emission_count, k=1):\n",
        "\tprint(\"Evaluating..\")\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\n",
        "\tall_pred_tags = []\n",
        "\n",
        "\tsentence = []\n",
        "\tfor line in lines:\n",
        "\t\tif line != \"\\n\":\n",
        "\t\t\tsentence.append(line.strip())\n",
        "\t\telse:\n",
        "\t\t\tpred_tags = get_sentence_tag(sentence, tokens, emission_count, k)\n",
        "\t\t\tall_pred_tags += pred_tags + [\"\\n\"]\n",
        "\n",
        "\t\t\tsentence = []\n",
        "\n",
        "\treturn lines, all_pred_tags\n",
        "\n",
        "\n",
        "def write_output(filepath, lines, all_pred_tags):\n",
        "\tprint(\"s output..\")\n",
        "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
        "\t\tfor i in range(len(lines)):\n",
        "\t\t\tword = lines[i].strip()\n",
        "\n",
        "\t\t\tif word != \"\\n\":\n",
        "\t\t\t\ttag = all_pred_tags[i]\n",
        "\n",
        "\t\t\t\tif tag != \"\\n\":\n",
        "\t\t\t\t\tf.write(word + \" \" + tag)\n",
        "\t\t\t\tf.write(\"\\n\")\n",
        "\n",
        "\tprint(\"Output successfully written!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\troot_dir = \"./\"\n",
        "\tdatasets = [\"ES\", \"RU\"]\n",
        "\n",
        "\tfor dataset in datasets:\n",
        "\t\t# print(\"For dataset {}:\".format(dataset))\n",
        "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
        "\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
        "\n",
        "\t\t# Train\n",
        "\t\ttokens, emission_count = train(train_path)\n",
        "\n",
        "\t\t# Estimate emission parameters using MLE\n",
        "\t\n",
        "\t\t\n",
        "\t\t# Evaluate\n",
        "\t\tlines, all_pred_tags = evaluate(evaluation_path, tokens, emission_count, k=1)\n",
        "\n",
        "\t\t# Write output file\n",
        "\t\toutput_path = root_dir + \"{}/dev.p1.out\".format(dataset)\n",
        "\t\twrite_output(output_path, lines, all_pred_tags)\n",
        "\n",
        "\t\t# print(\"Dataset {} done.\".format(dataset))\n",
        "\n",
        "\tprint(\"Done for all datasets!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyDwgADZVLVx"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f83YJti1VLVx",
        "outputId": "8f2ab972-4cc7-4e3b-8bee-c2234827ad76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For dataset ES:\n",
            "Training..\n",
            "ES\n",
            "All words have a tag. Proceeding..\n",
            "Writing output..\n",
            "Output successfully written!\n",
            "For dataset RU:\n",
            "Training..\n",
            "RU\n",
            "All words have a tag. Proceeding..\n",
            "Writing output..\n",
            "Output successfully written!\n"
          ]
        }
      ],
      "source": [
        "#Part 2\n",
        "def train(filepath):\n",
        "\tprint(\"Training..\")\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\t\t\n",
        "\tstart = \"START\"\n",
        "\tstop = \"STOP\"\n",
        "\n",
        "\t# Set of all unique tokens in file\n",
        "\ttokens = []\n",
        "\t\n",
        "\t# Nested dictionary to keep track of emission count\n",
        "\t# {tag: {token: count} }\n",
        "\temission_count = {} \n",
        "\n",
        "\t# Iterate through file to update tokens and emission_count\n",
        "\tfor line in lines:\n",
        "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
        "\t\tif len(line_split) == 2:\n",
        "\t\t\ttoken = line_split[0]\n",
        "\t\t\ttag = line_split[1]\n",
        "\n",
        "\t\t\tif token not in tokens:\n",
        "\t\t\t\ttokens.append(token)\n",
        "\n",
        "\t\t\tif tag not in emission_count:\n",
        "\t\t\t\tnested_tag_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
        "\t\t\tif token not in nested_tag_dict:\n",
        "\t\t\t\tnested_tag_dict[token] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict[token] += 1\n",
        "\t\t\temission_count[tag] = nested_tag_dict\n",
        "\t\t\t\n",
        "\treturn tokens, emission_count\n",
        "\n",
        "\n",
        "def est_emission_param(emission_count, token, tag, k=1):\n",
        "\ttag_dict = emission_count[tag]\n",
        "\n",
        "\tif token != \"#UNK#\":\n",
        "\t\ta = tag_dict.get(token, 0)\n",
        "\telse:\n",
        "\t\ta = k \n",
        "\tb = sum(tag_dict.values()) + k\n",
        "\n",
        "\treturn a / b\n",
        "\n",
        "\n",
        "\n",
        "def transition(filepath):\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\t\t\n",
        "\tstart = \"START\"\n",
        "\tstop = \"STOP\"\n",
        "\n",
        "\t# Set of all unique tokens in file\n",
        "\ttokens = []\n",
        "\t# Nested dictionary to keep track of emission count\n",
        "\t# {tag: {token: count} }\n",
        "\tu = start\n",
        "\ttransition_count = {} \n",
        "\n",
        "\t# Iterate through file to update tokens and transition_count\n",
        "\tfor line in lines:\n",
        "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
        "\t\t\n",
        "\t\t#Case 1\n",
        "\t\t\n",
        "\t\tif len(line_split) == 2:\n",
        "\t\t\ttoken = line_split[0]\n",
        "\t\t\tv = line_split[1]\n",
        "\n",
        "\t\t\tif u not in transition_count:\n",
        "\t\t\t\tu_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\tu_dict = transition_count[u]\n",
        "\n",
        "\t\t\tif v in u_dict:\n",
        "\t\t\t\tu_dict[v] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tu_dict[v] = 1\n",
        "\n",
        "\t\t\ttransition_count[u] = u_dict\n",
        "\t\t\tu = v\n",
        "\n",
        "\t\t#Case 2\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tu_dict = transition_count[u]\n",
        "\t\t\tv = stop\n",
        "\n",
        "\t\t\tif v in u_dict:\n",
        "\t\t\t\tu_dict[v] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tu_dict[v] = 1\n",
        "\n",
        "\t\t\ttransition_count[u] = u_dict\n",
        "\t\t\tu = start\n",
        "\treturn transition_count\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\n",
        "def transition_param(transition_count, u, v):\n",
        "\t\n",
        "\n",
        "\tif u not in transition_count:\n",
        "\t\ta = 0\n",
        "\telse:\n",
        "\t\tu_dict = transition_count[u]\n",
        "\t\ta = u_dict.get(v,0)\n",
        "\t\tb = sum(u_dict.values())\n",
        "\n",
        "\treturn a / b\n",
        "\t\n",
        "\n",
        "def viterbi_forward(emissions, transitions, words, labels):\n",
        "\tn = len(labels)\n",
        "\tsmallest = -9999\n",
        "\t\n",
        "\tstates = list(transitions.keys())\n",
        "\tstates.remove(\"START\")\n",
        "\n",
        "\t# initialize score dict\n",
        "\tscores = {}\n",
        "\n",
        "\tscores[0] = {}\n",
        "\n",
        "\tfor v in states:\n",
        "\t\ttransition_fraction = transition_param(transitions, \"START\", v)\n",
        "\t\tif transition_fraction != 0:\n",
        "\t\t\ttrans = math.log(transition_fraction)\n",
        "\t\telse:\n",
        "\t\t\ttrans = smallest\n",
        "\n",
        "\t\tif labels[0] not in words:\n",
        "\t\t\ttoken = \"#UNK#\"\n",
        "\t\telse:\n",
        "\t\t\ttoken = labels[0]\n",
        "\n",
        "\t\t# Emission Probability\n",
        "\t\tif ((token in emissions[v]) or (token == \"#UNK#\")): \n",
        "\t\t\temmision_fraction = est_emission_param(emissions, token, v)\n",
        "\t\t\temission = math.log(emmision_fraction)\n",
        "\t\telse:\n",
        "\t\t\temission = smallest\n",
        "\n",
        "\t\tstart = trans + emission\n",
        "\t\tscores[0][v] = (\"START\", start)\n",
        "        \n",
        "\t# State 1 to n\n",
        "\tfor i in range(1, n):\n",
        "\n",
        "\t\tscores[i] = {}\n",
        "\t\tfor v in states:\n",
        "\t\t\tfindmax = []\n",
        "\n",
        "\t\t\tfor u in states:\n",
        "\t\t\t\t# Transition Probability\n",
        "\t\t\t\ttransition_fraction = transition_param(transitions, u, v)\n",
        "\n",
        "\t\t\t\tif transition_fraction != 0:\n",
        "\t\t\t\t\ttrans = math.log(transition_fraction)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ttrans = smallest\n",
        "\t\t\t# if the word does not exist, assign special token\n",
        "\t\t\t\tif labels[i] not in words:\n",
        "\t\t\t\t\ttoken = \"#UNK#\"\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ttoken = labels[i]\n",
        "\n",
        "\t\t\t\t# Emission Probability\n",
        "\t\t\t\tif ((token in emissions[v]) or token == \"#UNK#\"):\n",
        "\t\t\t\t\temission_fraction = est_emission_param(emissions, token, v)\n",
        "\t\t\t\t\temission = math.log(emission_fraction)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\temission = smallest\n",
        "\t\t\t\t\t\n",
        "\t\t\t\tcurrent = scores[i-1][u][1] + trans + emission\n",
        "\t\t\t\tfindmax.append(current)\n",
        "    \n",
        "\n",
        "            # ARGMAX\n",
        "\t\t\tans = max(findmax)\n",
        "\t\t\tstate_ans = states[findmax.index(ans)]\n",
        "\t\t\tscores[i][v] = (state_ans, ans)\n",
        "\t\t\t\n",
        "\t# STATE N to Stop State\n",
        "\tscores[n] = {}\n",
        "\tstopmax = []\n",
        "\tfor u in states:\n",
        "        # Transition Probability\n",
        "\t\ttransition_fraction = transition_param(transitions, u, \"STOP\")\n",
        "\t\tif transition_fraction != 0:\n",
        "\t\t\ttransition = math.log(transition_fraction)\n",
        "\t\telse:\n",
        "\t\t\ttransition = smallest\n",
        "        \n",
        "\t\tstopscore = scores[n-1][u][1] + transition\n",
        "\t\tstopmax.append(stopscore)\n",
        "\t\t\n",
        "    \n",
        "\t# print(scores)\n",
        "    # ARGMAX\n",
        "\tstop = max(stopmax)\n",
        "\t# print(stop)\n",
        "\tstate_ans = states[stopmax.index(stop)]\n",
        "\t# print(states)\n",
        "\t# print(state_ans)\n",
        "\tscores[n] = (state_ans, stop)\n",
        "\t# print(scores)\n",
        "\t# Backtracking path\n",
        "\tpath = [\"STOP\"]\n",
        "    # scores[n] = ('O', -308.32462005568965)\n",
        "\tlast = scores[n][0]\n",
        "\tpath.insert(0, last)\n",
        "\t\n",
        "\tfor k in range(n-1, -1, -1):\n",
        "\t\tlast = scores[k][last][0]\n",
        "\t\t\n",
        "\t\tpath.insert(0, last)\n",
        "\t\t# print(path)\n",
        "\treturn path\n",
        "\t\n",
        "def write_output(filepath, lines, all_pred_tags):\n",
        "\tprint(\"Writing output..\")\n",
        "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
        "\t\tfor j in range(len(lines)):\n",
        "\t\t\tword = lines[j].strip()\n",
        "\t\t\tif word != \"\\n\":\n",
        "\t\t\t\t# print(word)\n",
        "\t\t\t\ttag = all_pred_tags[j]\n",
        "\t\t\t\t# print(tag)\n",
        "\t\t\t\tif(tag != \"\\n\"):\n",
        "\t\t\t\t\tf.write(word + \" \" + tag)\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\n",
        "\tprint(\"Output successfully written!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\troot_dir = \"./\"\n",
        "\tdatasets = [\"ES\", \"RU\"]\n",
        "\n",
        "\tfor dataset in datasets:\n",
        "\t\tprint(\"For dataset {}:\".format(dataset))\n",
        "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
        "\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
        "\n",
        "\t\t# Train\n",
        "\t\ttransition_count = transition(train_path)\n",
        "\t\ttokens, emission_count = train(train_path)\n",
        "\n",
        "\t\twith open(evaluation_path, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "\t\t\tlines = f.readlines()\n",
        "\n",
        "\t\tlabels = []\n",
        "\t\tall_pred_tags = []\n",
        "\t\tprint(dataset)\n",
        "\n",
        "\t\tfor line in lines:    \n",
        "\t\t\tif line != \"\\n\":    \n",
        "\t\t\t\tline = line.strip()\n",
        "\t\t\t\tlabels.append(line)\n",
        "\t\t\telse:\n",
        "\t\t\t\t# print(emission_count)\n",
        "\t\t\t\tsentence_prediction = viterbi_forward(emission_count, transition_count, tokens, labels)\n",
        "\t\t\t\tsentence_prediction.remove(\"START\")\n",
        "\t\t\t\tsentence_prediction.remove(\"STOP\")\n",
        "\t\t\t\t# print(sentence_prediction)\n",
        "\t\t\t\tall_pred_tags = all_pred_tags + sentence_prediction\n",
        "\t\t\t\tall_pred_tags = all_pred_tags + [\"\\n\"]\n",
        "\t\t\t\t# print(all_pred_tags)\n",
        "\t\t\t\tlabels = []\n",
        "\t\t# print(\"length of lines:\", len(lines),\"lenth of all tags:\", len(all_pred_tags))\n",
        "\t\tassert len(lines) == len(all_pred_tags)\n",
        "\t\tprint(\"All words have a tag. Proceeding..\")\n",
        "\n",
        "\t\toutput_path = root_dir + \"{}/dev.p2.out\".format(dataset)\n",
        "\t\t# print(all_pred_tags)\n",
        "\t\twrite_output(output_path, lines, all_pred_tags)\n",
        "\t\t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzMeCld9VLVz"
      },
      "source": [
        "# Part 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "sh06CbxfVLVz",
        "outputId": "968faef0-fc5e-44e5-d11c-d121f6937b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ES\n",
            "All words have a tag. Proceeding..\n",
            "Writing output..\n",
            "Output successfully written!\n",
            "RU\n",
            "All words have a tag. Proceeding..\n",
            "Writing output..\n",
            "Output successfully written!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        "    tokens3 = set()\n",
        "    \n",
        "    emission_count = {}\n",
        "    \n",
        "    for line in lines:\n",
        "        line_split = line.strip().rsplit(\" \", 1)  \n",
        "        \n",
        "        if len(line_split) == 2:\n",
        "            token3 = line_split[0]\n",
        "            tag = line_split[1]        \n",
        "            tokens3.add(token3)\n",
        "    \n",
        "            \n",
        "            if tag in emission_count:\n",
        "               \n",
        "                nested_tag_dict = emission_count[tag]\n",
        "            else:\n",
        "                \n",
        "                nested_tag_dict = {}\n",
        "                \n",
        "            if token3 in nested_tag_dict:\n",
        "                nested_tag_dict[token3] = nested_tag_dict[token3] + 1\n",
        "            else:\n",
        "                nested_tag_dict[token3] = 1\n",
        "            \n",
        "            emission_count[tag] = nested_tag_dict\n",
        "    \n",
        "    return tokens3, emission_count\n",
        "\n",
        "def est_emission_param(emission_count, token, tag, k = 1):\n",
        "    tag_dict = emission_count[tag]\n",
        "    \n",
        "    b = sum(tag_dict.values()) + k\n",
        "    \n",
        "    if token != \"#UNK#\":\n",
        "        a = tag_dict[token]\n",
        "    else: \n",
        "        a = k\n",
        "    \n",
        "    return a / b\n",
        "\n",
        "def tag_producer(emission_count, labels, tokens3):\n",
        "    tag_output = []\n",
        "    \n",
        "    for i in labels:\n",
        "        predicted_state = \"\"\n",
        "        highest_prob = -9999999.0\n",
        "        for tag in emission_count:\n",
        "            if i not in tokens3:\n",
        "                i = \"#UNK#\"\n",
        "                \n",
        "            if ((i in emission_count[tag]) or (i == \"#UNK#\")):\n",
        "                emission_prob = est_emission_param(emission_count, i, tag, 1)\n",
        "\n",
        "                if emission_prob > highest_prob:\n",
        "                    highest_prob = emission_prob\n",
        "                    predicted_state = tag\n",
        "                    \n",
        "        tag_output.append(predicted_state)\n",
        "    return tag_output\n",
        "        \n",
        "\n",
        "def transition(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    start = \"START\"\n",
        "    stop = \"STOP\"\n",
        "    \n",
        "    u = start\n",
        "    \n",
        "    transition_count = {}\n",
        "    \n",
        "    for line in lines:\n",
        "        line_split = line.strip().rsplit(\" \", 1)  \n",
        "        \n",
        "        # case 1\n",
        "        if len(line_split) == 2:\n",
        "            token3 = line_split[0]\n",
        "            v = line_split[1]\n",
        "            if u not in transition_count:\n",
        "                u_dict = {}\n",
        "            else:\n",
        "                u_dict = transition_count[u]\n",
        "            \n",
        "            if v in u_dict:\n",
        "                u_dict[v] += 1\n",
        "            else:\n",
        "                u_dict[v] = 1\n",
        "            transition_count[u] = u_dict\n",
        "            u = v\n",
        "            \n",
        "        if len(line_split) != 2:\n",
        "            u_dict = transition_count[u]\n",
        "            v = stop\n",
        "            if v in u_dict:\n",
        "                u_dict[v] += 1\n",
        "            else:\n",
        "                u_dict[v] = 1\n",
        "            transition_count[u] = u_dict\n",
        "            \n",
        "            u = start\n",
        "            \n",
        "            \n",
        "    return transition_count\n",
        "\n",
        "def transition_para(transition_count, u, v):\n",
        "    if u not in transition_count:\n",
        "        a = 0\n",
        "        \n",
        "    else:\n",
        "        u_dict = transition_count[u]\n",
        "    \n",
        "        a = u_dict.get(v, 0)\n",
        "    \n",
        "        b = sum(u_dict.values())\n",
        "        \n",
        "    \n",
        "    return a/b\n",
        "\n",
        "def viterbi_forward(N, emissions, transitions, words, labels):\n",
        "    n = len(labels)\n",
        "    smallest = -9999999\n",
        "\n",
        "    states = list(transitions.keys())\n",
        "    states.remove(\"START\")\n",
        "\n",
        "    scores = {}\n",
        "    scores[0] = {}\n",
        "\n",
        "    for v in states:\n",
        "        # Transition Probability\n",
        "        transition_fraction = transition_para(transitions, \"START\", v)\n",
        "        if transition_fraction != 0:\n",
        "            trans = math.log(transition_fraction)\n",
        "        else:\n",
        "            trans = smallest\n",
        "        \n",
        "        if labels[0] not in words:\n",
        "            token3 = \"#UNK#\"\n",
        "        else:\n",
        "            token3 = labels[0]\n",
        "\n",
        "        # Emission Probability\n",
        "        if ((token3 in emissions[v]) or (token3 == \"#UNK#\")): \n",
        "            emmision_fraction = est_emission_param(emissions, token3, v)\n",
        "            emission = math.log(emmision_fraction)\n",
        "        else:\n",
        "            emission = smallest\n",
        "        \n",
        "        start = trans + emission\n",
        "        scores[0][v] = (\"START\", start)\n",
        "\n",
        "    copyscores = copy.deepcopy(scores)\n",
        "    \n",
        "    # State 1 to n\n",
        "    for i in range(1, n):\n",
        "        scores[i] = {}\n",
        "        copyscores[i] = {}\n",
        "        for v in states:\n",
        "            findmax = []\n",
        "            for u in states:\n",
        "                # Transition Probability\n",
        "                transition_fraction = transition_para(transitions, u, v)\n",
        "                if transition_fraction != 0:\n",
        "                    trans = math.log(transition_fraction)\n",
        "                else:\n",
        "                    trans = smallest\n",
        "                if labels[i] not in words:\n",
        "                    v_v = \"#UNK#\"\n",
        "                else:\n",
        "                    v_v = labels[i]\n",
        "\n",
        "                # Emission Probability\n",
        "                if ((v_v in emissions[v]) or (v_v == \"#UNK#\")): \n",
        "                    emmision_fraction = est_emission_param(emissions, v_v, v)\n",
        "                    emission = math.log(emmision_fraction)\n",
        "                else:\n",
        "                    emission = smallest\n",
        "              \n",
        "                if i == 1 :\n",
        "                  currentscore = scores[i-1][u][1] + trans + emission\n",
        "                  findmax.append(currentscore)\n",
        "                else:\n",
        "                  currentscores = [[scores[i-1][u][m][1] for m in range(N)][j] + trans + emission for j in range(N)] # currentscores = [bestscore, 2nd bestscore, 3rd bestscore]\n",
        "                  for score in currentscores:\n",
        "                    findmax.append(score)\n",
        "            ans = [] \n",
        "            state_ans = []\n",
        "            copyfindmax = copy.deepcopy(findmax)\n",
        "            for m in range(N):\n",
        "                ans.append(max(copyfindmax))\n",
        "                state_ans.append(states[findmax.index(ans[m]) // N])\n",
        "                copyfindmax[findmax.index(ans[m])] = -999999999.999\n",
        "            scores[i][v] = tuple((state_ans[m], ans[m]) for m in range(N))\n",
        "            \n",
        "\n",
        "    # STOP STATE\n",
        "    scores[n] = {}\n",
        "    copyscores[n] = {}\n",
        "    stopmax = []\n",
        "\n",
        "    for u in states:\n",
        "        # Transition Probability\n",
        "        transition_fraction = transition_para(transitions, u, \"STOP\")\n",
        "        if transition_fraction != 0:\n",
        "            trans = math.log(transition_fraction)\n",
        "        else:\n",
        "            trans = smallest\n",
        "\n",
        "        if(type(scores[n-1][u][0])==tuple):\n",
        "            stopscore = [[scores[n-1][u][m][1] for m in range(N)][j] + trans + emission for j in range(N)]\n",
        "        else:\n",
        "            t=scores[n-1][u]\n",
        "            stopscore = [t[1]+ trans + emission]    \n",
        "            \n",
        "        for score in stopscore:\n",
        "            stopmax.append(score)\n",
        "            \n",
        "\n",
        "    stop = []\n",
        "    state_ans = []\n",
        "    copystopmax = copy.deepcopy(stopmax)\n",
        "    for i in range(N):\n",
        "        stop.append(max(copystopmax))\n",
        "        state_ans.append(states[stopmax.index(stop[i]) // N])\n",
        "        copystopmax[stopmax.index(stop[i])] = -999999999.999\n",
        "    scores[n][u] = tuple((state_ans[m], stop[m]) for m in range(N))\n",
        "    \n",
        "      \n",
        "    N_bestPaths = []\n",
        "    lasts = [] \n",
        "    for i in range(N):\n",
        "      path = [\"STOP\"]\n",
        "      last = list(scores[n].values())[0][i][0]\n",
        "      lasts.append(last)\n",
        "      path.insert(0, last)\n",
        "      N_bestPaths.append(path)\n",
        "    \n",
        "    for i in range(N):\n",
        "        for k in range(n-1, -1, -1):\n",
        "            if k == 0:\n",
        "                last = scores[k][N_bestPaths[i][0]][0] \n",
        "            else:\n",
        "                last = scores[k][N_bestPaths[i][0]][0][0]\n",
        "            N_bestPaths[i].insert(0, last)\n",
        "    \n",
        "    \n",
        "    return N_bestPaths[N-1]\n",
        "\n",
        "def write_output(filepath, lines, all_pred_tags):\n",
        "\tprint(\"Writing output..\")\n",
        "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
        "\t\tfor j in range(len(lines)):\n",
        "\t\t\tword = lines[j].strip()\n",
        "\t\t\tif word != \"\\n\":\n",
        "\t\t\t\t# print(word)\n",
        "\t\t\t\ttag = all_pred_tags[j]\n",
        "\t\t\t\t# print(tag)\n",
        "\t\t\t\tif(tag != \"\\n\"):\n",
        "\t\t\t\t\tf.write(word + \" \" + tag)\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\n",
        "\tprint(\"Output successfully written!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    root_dir = \"./\"\n",
        "\n",
        "    datasets = [\"ES\", \"RU\"]\n",
        "\n",
        "    for dataset in datasets:\n",
        "\n",
        "        train_path = root_dir + \"{}/train\".format(dataset)\n",
        "        evaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
        "        \n",
        "        # training\n",
        "        transition_count = transition(train_path)\n",
        "\n",
        "        tokens3, emission_count = train(train_path)\n",
        "        \n",
        "        # evaluation\n",
        "        with open(evaluation_path, \"r\", encoding='utf8') as f:\n",
        "            lines = f.readlines()\n",
        "        labels = []\n",
        "        \n",
        "        all_pred_tags = []\n",
        "        print(dataset)\n",
        "        N = 5\n",
        "        for line in lines:    \n",
        "            if line != \"\\n\":    \n",
        "                line = line.strip()\n",
        "                labels.append(line)\n",
        "            else:\n",
        "                # print(emission_count)\n",
        "                sentence_prediction = viterbi_forward(N, emission_count, transition_count, tokens3, labels)\n",
        "                sentence_prediction.remove(\"START\")\n",
        "                sentence_prediction.remove(\"STOP\")\n",
        "                # print(sentence_prediction)\n",
        "                all_pred_tags = all_pred_tags + sentence_prediction\n",
        "                all_pred_tags = all_pred_tags + [\"\\n\"]\n",
        "                # print(all_pred_tags)\n",
        "                labels = []\n",
        "        # print(\"length of lines:\", len(lines),\"lenth of all tags:\", len(all_pred_tags))\n",
        "        assert len(lines) == len(all_pred_tags)\n",
        "        print(\"All words have a tag. Proceeding..\")\n",
        "\n",
        "        output_path = root_dir + \"{}/dev.p3.out\".format(dataset)\n",
        "        # print(all_pred_tags)\n",
        "        write_output(output_path, lines, all_pred_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 4\n",
        "\n",
        "def train(filepath):\n",
        "\tprint(\"Training..\")\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\t\t\n",
        "\tstart = \"START\"\n",
        "\tstop = \"STOP\"\n",
        "\n",
        "\t# Set of all unique tokens in file\n",
        "\ttokens = []\n",
        "\t\n",
        "\t# Nested dictionary to keep track of emission count\n",
        "\t# {tag: {token: count} }\n",
        "\temission_count = {} \n",
        "\n",
        "\t# Iterate through file to update tokens and emission_count\n",
        "\tfor line in lines:\n",
        "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
        "\t\tif len(line_split) == 2:\n",
        "\t\t\ttoken = line_split[0]\n",
        "\t\t\ttag = line_split[1]\n",
        "\n",
        "\t\t\tif token not in tokens:\n",
        "\t\t\t\ttokens.append(token)\n",
        "\n",
        "\t\t\tif tag not in emission_count:\n",
        "\t\t\t\tnested_tag_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
        "\t\t\tif token not in nested_tag_dict:\n",
        "\t\t\t\tnested_tag_dict[token] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict[token] += 1\n",
        "\t\t\temission_count[tag] = nested_tag_dict\n",
        "\t\t\t\n",
        "\treturn tokens, emission_count\n",
        "\n",
        "\n",
        "def est_emission_param(emission_count, token, tag, k=1):\n",
        "\ttag_dict = emission_count[tag]\n",
        "\n",
        "\tif token != \"#UNK#\":\n",
        "\t\ta = tag_dict.get(token, 0)\n",
        "\telse:\n",
        "\t\ta = k \n",
        "\tb = sum(tag_dict.values()) + k\n",
        "\n",
        "\treturn a / b\n",
        "\n",
        "def transition(filepath):\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\t\t\n",
        "\tstart = \"START\"\n",
        "\tstop = \"STOP\"\n",
        "\ttokens = []\n",
        "\tu = start\n",
        "\tv = start\n",
        "\ttransition_count = {} \n",
        "\n",
        "\tfor line in lines:\n",
        "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
        "\t\t\n",
        "\t\t#Case 1\n",
        "\t\t\n",
        "\t\tif len(line_split) == 2:\n",
        "\t\t\ttoken = line_split[0]\n",
        "\t\t\tw = line_split[1]\n",
        "\n",
        "\t\t\tif u not in transition_count:\n",
        "\t\t\t\ttransition_count[u] = {}\n",
        "\t\t\t# else:\n",
        "\t\t\t# \tu_dict = transition_count[u]\n",
        "\t\t\t\n",
        "\t\t\tif v not in transition_count:\n",
        "\t\t\t\tv_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\tv_dict = transition_count[u][v]\n",
        "\n",
        "\t\t\tif w in v_dict:\n",
        "\t\t\t\tv_dict[w] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tv_dict[w] = 1\n",
        "\n",
        "\t\t\ttransition_count[u][v] = v_dict\n",
        "\t\t\tu = v\n",
        "\t\t\tv = w\n",
        "\n",
        "\t\t#Case 2\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tif u not in transition_count:\n",
        "\t\t\t\ttransition_count[u] = {}\n",
        "\t\t\t\n",
        "\t\t\tif v not in transition_count[u]:\n",
        "\t\t\t\tv_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\t# nested dictionary of current sentence\n",
        "\t\t\t\tv_dict = transition_count[u][v]\n",
        "\n",
        "\t\t\tstate_w = stop\n",
        "\n",
        "\t\t\tif state_w in v_dict:\n",
        "\t\t\t\tv_dict[state_w] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tv_dict[state_w] = 1\n",
        "\n",
        "\n",
        "\t\t\ttransition_count[u][v] = v_dict \n",
        "\n",
        "\t\t\tu = start\n",
        "\t\t\tv = stop\n",
        "\n",
        "\treturn transition_count\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3360a53796541dd2977b4996484fb33c50c413f15d574747a99ea8c321d96daf"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
