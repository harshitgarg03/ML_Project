{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Libraries\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1\n",
    "def train(filepath):\n",
    "\tprint(\"Training..\")\n",
    "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\n",
    "\t# Set of all unique tokens in file\n",
    "\ttokens = []\n",
    "\t# Nested dictionary to keep track of emission count\n",
    "\t# {tag: {token: count} }\n",
    "\temission_count = {} \n",
    "\n",
    "\t# Iterate through file to update tokens and emission_count\n",
    "\tfor line in lines:\n",
    "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
    "\t\tif len(line_split) == 2:\n",
    "\t\t\ttoken = line_split[0]\n",
    "\t\t\ttag = line_split[1]\n",
    "\n",
    "\t\t\tif token not in tokens:\n",
    "\t\t\t\ttokens.append(token)\n",
    "\n",
    "\t\t\tif tag not in emission_count:\n",
    "\t\t\t\tnested_tag_dict = {}\n",
    "\t\t\telse:\n",
    "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
    "\t\t\tif token not in nested_tag_dict:\n",
    "\t\t\t\tnested_tag_dict[token] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tnested_tag_dict[token] += 1\n",
    "\t\t\temission_count[tag] = nested_tag_dict\n",
    "\n",
    "\treturn tokens, emission_count\n",
    "\n",
    "\n",
    "def est_emission_param(emission_count, token, tag):\n",
    "\ttag_dict = emission_count[tag]\n",
    "\n",
    "\ta = tag_dict.get(token, 0)\t# Returns 0 if none\n",
    "\tb = sum(tag_dict.values())\n",
    "\n",
    "\treturn a / b\n",
    "\n",
    "\n",
    "def est_emission_param(emission_count, token, tag, k=1):\n",
    "\ttag_dict = emission_count[tag]\n",
    "\n",
    "\tif token != \"#UNK#\":\n",
    "\t\ta = tag_dict.get(token, 0)\n",
    "\telse:\n",
    "\t\ta = k \n",
    "\tb = sum(tag_dict.values()) + k\n",
    "\n",
    "\treturn a / b\n",
    "\n",
    "\n",
    "def get_sentence_tag(sentence, tokens, emission_count, k=1):\n",
    "\tpred_tags = []\n",
    "\n",
    "\tfor word in sentence:\n",
    "\t\tpred_tag = \"\"\n",
    "\t\tmax_emission = float('-inf')\n",
    "\n",
    "\t\tfor tag in emission_count:\n",
    "\t\t\tif word not in tokens:\n",
    "\t\t\t\tword = \"#UNK#\"\n",
    "\n",
    "\t\t\tif word in emission_count[tag] or word == \"#UNK#\":\n",
    "\t\t\t\temission = est_emission_param(emission_count, word, tag, k)\n",
    "\t\t\t\tif emission > max_emission:\n",
    "\t\t\t\t\tpred_tag = tag \n",
    "\t\t\t\t\tmax_emission = emission\n",
    "\n",
    "\t\tpred_tags.append(pred_tag)\n",
    "\n",
    "\treturn pred_tags\n",
    "\n",
    "\n",
    "def evaluate(filepath, tokens, emission_count, k=1):\n",
    "\tprint(\"Evaluating..\")\n",
    "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\n",
    "\tall_pred_tags = []\n",
    "\n",
    "\tsentence = []\n",
    "\tfor line in lines:\n",
    "\t\tif line != \"\\n\":\n",
    "\t\t\tsentence.append(line.strip())\n",
    "\t\telse:\n",
    "\t\t\tpred_tags = get_sentence_tag(sentence, tokens, emission_count, k)\n",
    "\t\t\tall_pred_tags += pred_tags + [\"\\n\"]\n",
    "\n",
    "\t\t\tsentence = []\n",
    "\n",
    "\treturn lines, all_pred_tags\n",
    "\n",
    "\n",
    "def write_output(filepath, lines, all_pred_tags):\n",
    "\tprint(\"Writing output..\")\n",
    "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
    "\t\tfor i in range(len(lines)):\n",
    "\t\t\tword = lines[i].strip()\n",
    "\n",
    "\t\t\tif word != \"\\n\":\n",
    "\t\t\t\ttag = all_pred_tags[i]\n",
    "\n",
    "\t\t\t\tif tag != \"\\n\":\n",
    "\t\t\t\t\tf.write(word + \" \" + tag)\n",
    "\t\t\t\tf.write(\"\\n\")\n",
    "\n",
    "\tprint(\"Output successfully written!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\troot_dir = \"./\"\n",
    "\tdatasets = [\"ES\", \"RU\"]\n",
    "\n",
    "\tfor dataset in datasets:\n",
    "\t\tprint(\"For dataset {}:\".format(dataset))\n",
    "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
    "\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
    "\n",
    "\t\t# Train\n",
    "\t\ttokens, emission_count = train(train_path)\n",
    "\n",
    "\t\t# Estimate emission parameters using MLE\n",
    "\t\t\"\"\"\n",
    "\t\tfor tag in emission_count:\n",
    "\t\t\tfor token in tokens:\n",
    "\t\t\t\t#emission = est_emission_param(emission_count, token, tag)\t\t# Without tackling special word token\n",
    "\t\t\t\temission = est_emission_param(emission_count, token, tag, k=1)\t# With special word token\n",
    "\t\t\t\tif emission != 0:\n",
    "\t\t\t\t\tprint(\"Emission parameters for {x} given {y}: {emission}\".format(x=token, y=tag, emission=emission))\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\t# Evaluate\n",
    "\t\tlines, all_pred_tags = evaluate(evaluation_path, tokens, emission_count, k=1)\n",
    "\n",
    "\t\t# Write output file\n",
    "\t\toutput_path = root_dir + \"{}/dev.p1.out\".format(dataset)\n",
    "\t\twrite_output(output_path, lines, all_pred_tags)\n",
    "\n",
    "\t\tprint(\"Dataset {} done.\".format(dataset))\n",
    "\n",
    "\tprint(\"Done for all datasets!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "def train(filepath):\n",
    "\tprint(\"Training..\")\n",
    "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\t\n",
    "\tstart = \"START\"\n",
    "\tstop = \"STOP\"\n",
    "\n",
    "\t# Set of all unique tokens in file\n",
    "\ttokens = []\n",
    "\t# Nested dictionary to keep track of emission count\n",
    "\t# {tag: {token: count} }\n",
    "\temission_count = {} \n",
    "\n",
    "\t# Iterate through file to update tokens and emission_count\n",
    "\tfor line in lines:\n",
    "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
    "\t\tif len(line_split) == 2:\n",
    "\t\t\ttoken = line_split[0]\n",
    "\t\t\ttag = line_split[1]\n",
    "\n",
    "\t\t\tif token not in tokens:\n",
    "\t\t\t\ttokens.append(token)\n",
    "\n",
    "\t\t\tif tag not in emission_count:\n",
    "\t\t\t\tnested_tag_dict = {}\n",
    "\t\t\telse:\n",
    "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
    "\t\t\tif token not in nested_tag_dict:\n",
    "\t\t\t\tnested_tag_dict[token] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tnested_tag_dict[token] += 1\n",
    "\t\t\temission_count[tag] = nested_tag_dict\n",
    "\t\t\t\n",
    "\treturn tokens, emission_count\n",
    "\n",
    "\n",
    "def est_emission_param(emission_count, token, tag, k=1):\n",
    "\ttag_dict = emission_count[tag]\n",
    "\n",
    "\tif token != \"#UNK#\":\n",
    "\t\ta = tag_dict.get(token, 0)\n",
    "\telse:\n",
    "\t\ta = k \n",
    "\tb = sum(tag_dict.values()) + k\n",
    "\n",
    "\treturn a / b\n",
    "\n",
    "\n",
    "def transition(filepath):\n",
    "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\t\n",
    "\tstart = \"START\"\n",
    "\tstop = \"STOP\"\n",
    "\n",
    "\t# Set of all unique tokens in file\n",
    "\ttokens = []\n",
    "\t# Nested dictionary to keep track of emission count\n",
    "\t# {tag: {token: count} }\n",
    "\tu = \"START\"\n",
    "\temission_count = {} \n",
    "\n",
    "\t# Iterate through file to update tokens and emission_count\n",
    "\tfor line in lines:\n",
    "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
    "\t\t\n",
    "\t\t#Case 1\n",
    "\t\t\n",
    "\t\tif len(line_split) == 2:\n",
    "\t\t\ttoken = line_split[0]\n",
    "\t\t\tv = line_split[1]\n",
    "\n",
    "\t\t\tif u not in emission_count:\n",
    "\t\t\t\tu_dict = {}\n",
    "\t\t\telse:\n",
    "\t\t\t\tu_dict = emission_count[u]\n",
    "\n",
    "\t\t\tif v in u_dict:\n",
    "\t\t\t\tu_dict[v] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tu_dict[v] = 1\n",
    "\n",
    "\t\t\temission_count[u] = u_dict\n",
    "\t\t\tu = v\n",
    "\n",
    "\t\t#Case 2\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\tu_dict = emission_count[u]\n",
    "\t\t\tv = stop\n",
    "\n",
    "\t\t\tif v in u_dict:\n",
    "\t\t\t\tu_dict[v] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tu_dict[v] = 1\n",
    "\n",
    "\t\t\temission_count[u] = u_dict\n",
    "\t\t\tu = start\n",
    "\t\t\t\n",
    "\treturn emission_count\n",
    "\n",
    "def transition_param(emission_count, u, v):\n",
    "\t\n",
    "\n",
    "\tif u not in emission_count:\n",
    "\t\ta = 0\n",
    "\telse:\n",
    "\t\tu_dict = emission_count[u]\n",
    "\t\ta = u_dict.get(v,0)\n",
    "\t\tb = sum(u_dict.values())\n",
    "\n",
    "\treturn a / b\n",
    "\n",
    "def viterbi_forward(emissions, transitions, words, labels):\n",
    "\tn = len(labels)\n",
    "\tsmallest = float('-inf')\n",
    "\t\n",
    "\tstates = list(transitions.keys())\n",
    "    states.remove(\"START\")\n",
    "\n",
    "\tfor v in states:\n",
    "\t\ttransition_fraction = transition_param(transitions, \"START\", v)\n",
    "\t\tif transition_fraction != 0:\n",
    "            trans = math.log(trans_frac)\n",
    "        else:\n",
    "            trans = smallest\n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "def transition(data):\n",
    "    # Initialise variables\n",
    "    START = \"START\"\n",
    "    STOP = \"STOP\"\n",
    "    count_u_to_v = {}\n",
    "    count_y = {}\n",
    "\n",
    "    # Generate the sequences from data together with START and STOP state\n",
    "    sentences = []\n",
    "    sequence = []\n",
    "    for idx, line in enumerate(data):\n",
    "        print(idx, line)\n",
    "        if not line[0]: # line is empty\n",
    "            sequence = [START] + sequence + [STOP]\n",
    "            sentences.append(sequence)\n",
    "            sequence = []\n",
    "        else:\n",
    "            sequence.append(line[1])\n",
    "\n",
    "    # Counting\n",
    "    num_sentences = len(sentences)\n",
    "    count_y[START] = num_sentences\n",
    "    count_y[STOP] = num_sentences\n",
    "\n",
    "    for seq in sentences:\n",
    "        prev_y = START\n",
    "        for curr_y in seq[1:]:\n",
    "            key = (prev_y, curr_y)\n",
    "\n",
    "            if key not in count_u_to_v:\n",
    "                count_u_to_v[(prev_y,curr_y)] = 1\n",
    "            elif key in count_u_to_v:\n",
    "                count_u_to_v[(prev_y,curr_y)] += 1\n",
    "\n",
    "            if curr_y != STOP:\n",
    "                if curr_y not in count_y:\n",
    "                    count_y[curr_y] = 1\n",
    "                elif curr_y in count_y:\n",
    "                    count_y[curr_y] += 1\n",
    "            prev_y = curr_y\n",
    "            \n",
    "    transitions = {}\n",
    "    for (u,v), count in count_u_to_v.items():\n",
    "        transitions[(u,v)] = count/count_y[u]\n",
    "\n",
    "    return transitions\n",
    "\n",
    "with open(\"ES/train\", \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = [line.strip(\"\\n\").split(\" \") for line in f.readlines()]\n",
    "c=transition(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3360a53796541dd2977b4996484fb33c50c413f15d574747a99ea8c321d96daf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
