{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v_deA2IhVLVr"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import copy\n",
        "from operator import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGg_gEPTVLVu"
      },
      "source": [
        "# Part 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EYpW97VLVLVv",
        "outputId": "06e492d5-8e52-4ebf-a948-0f99b1b2d381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training..\n",
            "Evaluating..\n",
            "s output..\n",
            "Output successfully written!\n",
            "Training..\n",
            "Evaluating..\n",
            "s output..\n",
            "Output successfully written!\n",
            "Done for all datasets!!\n"
          ]
        }
      ],
      "source": [
        "#Part 1\n",
        "def train(filepath):\n",
        "\tprint(\"Training..\")\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\n",
        "\t# Set of all unique tokens in file\n",
        "\ttokens = []\n",
        "\t# Nested dictionary to keep track of emission count\n",
        "\t# {tag: {token: count} }\n",
        "\temission_count = {} \n",
        "\n",
        "\t# Iterate through file to update tokens and emission_count\n",
        "\tfor line in lines:\n",
        "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
        "\t\tif len(line_split) == 2:\n",
        "\t\t\ttoken = line_split[0]\n",
        "\t\t\ttag = line_split[1]\n",
        "\n",
        "\t\t\tif token not in tokens:\n",
        "\t\t\t\ttokens.append(token)\n",
        "\n",
        "\t\t\tif tag not in emission_count:\n",
        "\t\t\t\tnested_tag_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
        "\t\t\tif token not in nested_tag_dict:\n",
        "\t\t\t\tnested_tag_dict[token] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict[token] += 1\n",
        "\t\t\temission_count[tag] = nested_tag_dict\n",
        "\n",
        "\treturn tokens, emission_count\n",
        "\n",
        "\n",
        "def est_emission_param(emission_count, token, tag):\n",
        "\ttag_dict = emission_count[tag]\n",
        "\n",
        "\ta = tag_dict.get(token, 0)\t# Returns 0 if none\n",
        "\tb = sum(tag_dict.values())\n",
        "\n",
        "\treturn a / b\n",
        "\n",
        "\n",
        "def est_emission_param(emission_count, token, tag, k=1):\n",
        "\ttag_dict = emission_count[tag]\n",
        "\n",
        "\tif token != \"#UNK#\":\n",
        "\t\ta = tag_dict.get(token, 0)\n",
        "\telse:\n",
        "\t\ta = k \n",
        "\tb = sum(tag_dict.values()) + k\n",
        "\n",
        "\treturn a / b\n",
        "\n",
        "\n",
        "def get_sentence_tag(sentence, tokens, emission_count, k=1):\n",
        "\tpred_tags = []\n",
        "\n",
        "\tfor word in sentence:\n",
        "\t\tpred_tag = \"\"\n",
        "\t\tmax_emission = float('-inf')\n",
        "\n",
        "\t\tfor tag in emission_count:\n",
        "\t\t\tif word not in tokens:\n",
        "\t\t\t\tword = \"#UNK#\"\n",
        "\n",
        "\t\t\tif word in emission_count[tag] or word == \"#UNK#\":\n",
        "\t\t\t\temission = est_emission_param(emission_count, word, tag, k)\n",
        "\t\t\t\tif emission > max_emission:\n",
        "\t\t\t\t\tpred_tag = tag \n",
        "\t\t\t\t\tmax_emission = emission\n",
        "\n",
        "\t\tpred_tags.append(pred_tag)\n",
        "\n",
        "\treturn pred_tags\n",
        "\n",
        "\n",
        "def evaluate(filepath, tokens, emission_count, k=1):\n",
        "\tprint(\"Evaluating..\")\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\n",
        "\tall_pred_tags = []\n",
        "\n",
        "\tsentence = []\n",
        "\tfor line in lines:\n",
        "\t\tif line != \"\\n\":\n",
        "\t\t\tsentence.append(line.strip())\n",
        "\t\telse:\n",
        "\t\t\tpred_tags = get_sentence_tag(sentence, tokens, emission_count, k)\n",
        "\t\t\tall_pred_tags += pred_tags + [\"\\n\"]\n",
        "\n",
        "\t\t\tsentence = []\n",
        "\n",
        "\treturn lines, all_pred_tags\n",
        "\n",
        "\n",
        "def write_output(filepath, lines, all_pred_tags):\n",
        "\tprint(\"s output..\")\n",
        "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
        "\t\tfor i in range(len(lines)):\n",
        "\t\t\tword = lines[i].strip()\n",
        "\n",
        "\t\t\tif word != \"\\n\":\n",
        "\t\t\t\ttag = all_pred_tags[i]\n",
        "\n",
        "\t\t\t\tif tag != \"\\n\":\n",
        "\t\t\t\t\tf.write(word + \" \" + tag)\n",
        "\t\t\t\tf.write(\"\\n\")\n",
        "\n",
        "\tprint(\"Output successfully written!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\troot_dir = \"./\"\n",
        "\tdatasets = [\"ES\", \"RU\"]\n",
        "\n",
        "\tfor dataset in datasets:\n",
        "\t\t# print(\"For dataset {}:\".format(dataset))\n",
        "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
        "\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
        "\n",
        "\t\t# Train\n",
        "\t\ttokens, emission_count = train(train_path)\n",
        "\n",
        "\t\t# Estimate emission parameters using MLE\n",
        "\t\n",
        "\t\t\n",
        "\t\t# Evaluate\n",
        "\t\tlines, all_pred_tags = evaluate(evaluation_path, tokens, emission_count, k=1)\n",
        "\n",
        "\t\t# Write output file\n",
        "\t\toutput_path = root_dir + \"{}/dev.p1.out\".format(dataset)\n",
        "\t\twrite_output(output_path, lines, all_pred_tags)\n",
        "\n",
        "\t\t# print(\"Dataset {} done.\".format(dataset))\n",
        "\n",
        "\tprint(\"Done for all datasets!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyDwgADZVLVx"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f83YJti1VLVx",
        "outputId": "8f2ab972-4cc7-4e3b-8bee-c2234827ad76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For dataset ES:\n",
            "Training..\n",
            "ES\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-3-572809e1555c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    270\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                                 \u001b[1;31m# print(emission_count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m                                 \u001b[0msentence_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mviterbi_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m                                 \u001b[0msentence_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"START\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m                                 \u001b[0msentence_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"STOP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-3-572809e1555c>\u001b[0m in \u001b[0;36mviterbi_forward\u001b[1;34m(emissions, transitions, words, sentence)\u001b[0m\n\u001b[0;32m    177\u001b[0m                                 \u001b[1;31m# Emission Probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memissions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"#UNK#\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                                         \u001b[0memission_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest_emission_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m                                         \u001b[0memission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memission_fraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-3-572809e1555c>\u001b[0m in \u001b[0;36mest_emission_param\u001b[1;34m(emission_count, token, tag, k)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Part 2\n",
        "def train(filepath):\n",
        "\tprint(\"Training..\")\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\t\t\n",
        "\tstart = \"START\"\n",
        "\tstop = \"STOP\"\n",
        "\n",
        "\t# Set of all unique tokens in file\n",
        "\ttokens = []\n",
        "\t\n",
        "\t# Nested dictionary to keep track of emission count\n",
        "\t# {tag: {token: count} }\n",
        "\temission_count = {} \n",
        "\n",
        "\t# Iterate through file to update tokens and emission_count\n",
        "\tfor line in lines:\n",
        "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
        "\t\tif len(line_split) == 2:\n",
        "\t\t\ttoken = line_split[0]\n",
        "\t\t\ttag = line_split[1]\n",
        "\n",
        "\t\t\tif token not in tokens:\n",
        "\t\t\t\ttokens.append(token)\n",
        "\n",
        "\t\t\tif tag not in emission_count:\n",
        "\t\t\t\tnested_tag_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
        "\t\t\tif token not in nested_tag_dict:\n",
        "\t\t\t\tnested_tag_dict[token] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tnested_tag_dict[token] += 1\n",
        "\t\t\temission_count[tag] = nested_tag_dict\n",
        "\t\t\t\n",
        "\treturn tokens, emission_count\n",
        "\n",
        "\n",
        "def est_emission_param(emission_count, token, tag, k=1):\n",
        "\ttag_dict = emission_count[tag]\n",
        "\n",
        "\tif token != \"#UNK#\":\n",
        "\t\ta = tag_dict.get(token, 0)\n",
        "\telse:\n",
        "\t\ta = k \n",
        "\tb = sum(tag_dict.values()) + k\n",
        "\n",
        "\treturn a / b\n",
        "\n",
        "\n",
        "\n",
        "def transition(filepath):\n",
        "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
        "\t\tlines = f.readlines()\n",
        "\t\t\n",
        "\tstart = \"START\"\n",
        "\tstop = \"STOP\"\n",
        "\n",
        "\t# Set of all unique tokens in file\n",
        "\ttokens = []\n",
        "\t# Nested dictionary to keep track of emission count\n",
        "\t# {tag: {token: count} }\n",
        "\tu = start\n",
        "\ttransition_count = {} \n",
        "\n",
        "\t# Iterate through file to update tokens and transition_count\n",
        "\tfor line in lines:\n",
        "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
        "\t\t\n",
        "\t\t#Case 1\n",
        "\t\t\n",
        "\t\tif len(line_split) == 2:\n",
        "\t\t\ttoken = line_split[0]\n",
        "\t\t\tv = line_split[1]\n",
        "\n",
        "\t\t\tif u not in transition_count:\n",
        "\t\t\t\tu_dict = {}\n",
        "\t\t\telse:\n",
        "\t\t\t\tu_dict = transition_count[u]\n",
        "\n",
        "\t\t\tif v in u_dict:\n",
        "\t\t\t\tu_dict[v] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tu_dict[v] = 1\n",
        "\n",
        "\t\t\ttransition_count[u] = u_dict\n",
        "\t\t\tu = v\n",
        "\n",
        "\t\t#Case 2\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tu_dict = transition_count[u]\n",
        "\t\t\tv = stop\n",
        "\n",
        "\t\t\tif v in u_dict:\n",
        "\t\t\t\tu_dict[v] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tu_dict[v] = 1\n",
        "\n",
        "\t\t\ttransition_count[u] = u_dict\n",
        "\t\t\tu = start\n",
        "\treturn transition_count\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\n",
        "def transition_param(transition_count, u, v):\n",
        "\t\n",
        "\n",
        "\tif u not in transition_count:\n",
        "\t\ta = 0\n",
        "\telse:\n",
        "\t\tu_dict = transition_count[u]\n",
        "\t\ta = u_dict.get(v,0)\n",
        "\t\tb = sum(u_dict.values())\n",
        "\n",
        "\treturn a / b\n",
        "\t\n",
        "\n",
        "def viterbi_forward(emissions, transitions, words, sentence):\n",
        "\tn = len(sentence)\n",
        "\tsmallest = -6969420\n",
        "\t\n",
        "\tstates = list(transitions.keys())\n",
        "\tstates.remove(\"START\")\n",
        "\n",
        "\t# initialize score dict\n",
        "\tscores = {}\n",
        "\n",
        "\tscores[0] = {}\n",
        "\n",
        "\tfor v in states:\n",
        "\t\ttransition_fraction = transition_param(transitions, \"START\", v)\n",
        "\t\tif transition_fraction != 0:\n",
        "\t\t\ttrans = math.log(transition_fraction)\n",
        "\t\telse:\n",
        "\t\t\ttrans = smallest\n",
        "\n",
        "\t\tif sentence[0] not in words:\n",
        "\t\t\ttoken = \"#UNK#\"\n",
        "\t\telse:\n",
        "\t\t\ttoken = sentence[0]\n",
        "\n",
        "\t\t# Emission Probability\n",
        "\t\tif ((token in emissions[v]) or (token == \"#UNK#\")): \n",
        "\t\t\temmision_fraction = est_emission_param(emissions, token, v)\n",
        "\t\t\temission = math.log(emmision_fraction)\n",
        "\t\telse:\n",
        "\t\t\temission = smallest\n",
        "\n",
        "\t\tstart = trans + emission\n",
        "\t\tscores[0][v] = (\"START\", start)\n",
        "        \n",
        "\t# State 1 to n\n",
        "\tfor i in range(1, n):\n",
        "\n",
        "\t\tscores[i] = {}\n",
        "\t\tfor v in states:\n",
        "\t\t\tfindmax = []\n",
        "\n",
        "\t\t\tfor u in states:\n",
        "\t\t\t\t# Transition Probability\n",
        "\t\t\t\ttransition_fraction = transition_param(transitions, u, v)\n",
        "\n",
        "\t\t\t\tif transition_fraction != 0:\n",
        "\t\t\t\t\ttrans = math.log(transition_fraction)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ttrans = smallest\n",
        "\t\t\t# if the word does not exist, assign special token\n",
        "\t\t\t\tif sentence[i] not in words:\n",
        "\t\t\t\t\ttoken = \"#UNK#\"\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ttoken = sentence[i]\n",
        "\n",
        "\t\t\t\t# Emission Probability\n",
        "\t\t\t\tif ((token in emissions[v]) or token == \"#UNK#\"):\n",
        "\t\t\t\t\temission_fraction = est_emission_param(emissions, token, v)\n",
        "\t\t\t\t\temission = math.log(emission_fraction)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\temission = smallest\n",
        "\t\t\t\t\t\n",
        "\t\t\t\tcurrent = scores[i-1][u][1] + trans + emission\n",
        "\t\t\t\tfindmax.append(current)\n",
        "    \n",
        "\n",
        "            # ARGMAX\n",
        "\t\t\tans = max(findmax)\n",
        "\t\t\tstate_ans = states[findmax.index(ans)]\n",
        "\t\t\tscores[i][v] = (state_ans, ans)\n",
        "\t\t\t\n",
        "\t# STATE N to Stop State\n",
        "\tscores[n] = {}\n",
        "\tstopmax = []\n",
        "\tfor u in states:\n",
        "        # Transition Probability\n",
        "\t\ttransition_fraction = transition_param(transitions, u, \"STOP\")\n",
        "\t\tif transition_fraction != 0:\n",
        "\t\t\ttransition = math.log(transition_fraction)\n",
        "\t\telse:\n",
        "\t\t\ttransition = smallest\n",
        "        \n",
        "\t\tstopscore = scores[n-1][u][1] + transition\n",
        "\t\tstopmax.append(stopscore)\n",
        "\t\t\n",
        "    \n",
        "\t# print(scores)\n",
        "    # ARGMAX\n",
        "\tstop = max(stopmax)\n",
        "\t# print(stop)\n",
        "\tstate_ans = states[stopmax.index(stop)]\n",
        "\t# print(states)\n",
        "\t# print(state_ans)\n",
        "\tscores[n] = (state_ans, stop)\n",
        "\t# print(scores)\n",
        "\t# Backtracking path\n",
        "\tpath = [\"STOP\"]\n",
        "\tlast = scores[n][0]\n",
        "\tpath.insert(0, last)\n",
        "\t\n",
        "\tfor k in range(n-1, -1, -1):\n",
        "\t\tlast = scores[k][last][0]\n",
        "\t\t\n",
        "\t\tpath.insert(0, last)\n",
        "\t\t# print(path)\n",
        "\treturn path\n",
        "\t\n",
        "def write_output(filepath, lines, all_pred_tags):\n",
        "\tprint(\"Writing output..\")\n",
        "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
        "\t\tfor j in range(len(lines)):\n",
        "\t\t\tword = lines[j].strip()\n",
        "\t\t\tif word != \"\\n\":\n",
        "\t\t\t\t# print(word)\n",
        "\t\t\t\ttag = all_pred_tags[j]\n",
        "\t\t\t\t# print(tag)\n",
        "\t\t\t\tif(tag != \"\\n\"):\n",
        "\t\t\t\t\tf.write(word + \" \" + tag)\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\n",
        "\tprint(\"Output successfully written!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\troot_dir = \"./\"\n",
        "\tdatasets = [\"ES\", \"RU\"]\n",
        "\n",
        "\tfor dataset in datasets:\n",
        "\t\tprint(\"For dataset {}:\".format(dataset))\n",
        "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
        "\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
        "\n",
        "\t\t# Train\n",
        "\t\ttransition_count = transition(train_path)\n",
        "\t\ttokens, emission_count = train(train_path)\n",
        "\n",
        "\t\twith open(evaluation_path, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "\t\t\tlines = f.readlines()\n",
        "\n",
        "\t\tsentence = []\n",
        "\t\tall_pred_tags = []\n",
        "\t\tprint(dataset)\n",
        "\n",
        "\t\tfor line in lines:    \n",
        "\t\t\tif line != \"\\n\":    \n",
        "\t\t\t\tline = line.strip()\n",
        "\t\t\t\tsentence.append(line)\n",
        "\t\t\telse:\n",
        "\t\t\t\t# print(emission_count)\n",
        "\t\t\t\tsentence_prediction = viterbi_forward(emission_count, transition_count, tokens, sentence)\n",
        "\t\t\t\tsentence_prediction.remove(\"START\")\n",
        "\t\t\t\tsentence_prediction.remove(\"STOP\")\n",
        "\t\t\t\t# print(sentence_prediction)\n",
        "\t\t\t\tall_pred_tags = all_pred_tags + sentence_prediction\n",
        "\t\t\t\tall_pred_tags = all_pred_tags + [\"\\n\"]\n",
        "\t\t\t\t# print(all_pred_tags)\n",
        "\t\t\t\tsentence = []\n",
        "\t\t# print(\"length of lines:\", len(lines),\"lenth of all tags:\", len(all_pred_tags))\n",
        "\t\tassert len(lines) == len(all_pred_tags)\n",
        "\t\tprint(\"All words have a tag. Proceeding..\")\n",
        "\n",
        "\t\toutput_path = root_dir + \"{}/dev.p2.out\".format(dataset)\n",
        "\t\t# print(all_pred_tags)\n",
        "\t\twrite_output(output_path, lines, all_pred_tags)\n",
        "\t\t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzMeCld9VLVz"
      },
      "source": [
        "# Part 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "sh06CbxfVLVz",
        "outputId": "968faef0-fc5e-44e5-d11c-d121f6937b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ES\n",
            "All words have a tag. Proceeding..\n",
            "Writing output..\n",
            "Output successfully written!\n",
            "RU\n",
            "All words have a tag. Proceeding..\n",
            "Writing output..\n",
            "Output successfully written!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        "    tokens3 = set()\n",
        "    \n",
        "    emission_count = {}\n",
        "    \n",
        "    for line in lines:\n",
        "        line_split = line.strip().rsplit(\" \", 1)  \n",
        "        \n",
        "        if len(line_split) == 2:\n",
        "            token3 = line_split[0]\n",
        "            tag = line_split[1]        \n",
        "            tokens3.add(token3)\n",
        "    \n",
        "            \n",
        "            if tag in emission_count:\n",
        "               \n",
        "                nested_tag_dict = emission_count[tag]\n",
        "            else:\n",
        "                \n",
        "                nested_tag_dict = {}\n",
        "                \n",
        "            if token3 in nested_tag_dict:\n",
        "                nested_tag_dict[token3] = nested_tag_dict[token3] + 1\n",
        "            else:\n",
        "                nested_tag_dict[token3] = 1\n",
        "            \n",
        "            emission_count[tag] = nested_tag_dict\n",
        "    \n",
        "    return tokens3, emission_count\n",
        "\n",
        "def est_emission_param(emission_count, token, tag, k = 1):\n",
        "    tag_dict = emission_count[tag]\n",
        "    \n",
        "    b = sum(tag_dict.values()) + k\n",
        "    \n",
        "    if token != \"#UNK#\":\n",
        "        a = tag_dict[token]\n",
        "    else: \n",
        "        a = k\n",
        "    \n",
        "    return a / b\n",
        "\n",
        "def get_sentence_tag(sentence, tokens, emission_count, k=1):\n",
        "\tpred_tags = []\n",
        "\n",
        "\tfor word in sentence:\n",
        "\t\tpred_tag = \"\"\n",
        "\t\tmax_emission = float('-inf')\n",
        "\n",
        "\t\tfor tag in emission_count:\n",
        "\t\t\tif word not in tokens:\n",
        "\t\t\t\tword = \"#UNK#\"\n",
        "\n",
        "\t\t\tif word in emission_count[tag] or word == \"#UNK#\":\n",
        "\t\t\t\temission = est_emission_param(emission_count, word, tag, k)\n",
        "\t\t\t\tif emission > max_emission:\n",
        "\t\t\t\t\tpred_tag = tag \n",
        "\t\t\t\t\tmax_emission = emission\n",
        "\n",
        "\t\tpred_tags.append(pred_tag)\n",
        "\n",
        "\treturn pred_tags\n",
        "        \n",
        "\n",
        "def transition(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    start = \"START\"\n",
        "    stop = \"STOP\"\n",
        "    \n",
        "    u = start\n",
        "    \n",
        "    transition_count = {}\n",
        "    \n",
        "    for line in lines:\n",
        "        line_split = line.strip().rsplit(\" \", 1)  \n",
        "        \n",
        "        # case 1\n",
        "        if len(line_split) == 2:\n",
        "            token3 = line_split[0]\n",
        "            v = line_split[1]\n",
        "            if u not in transition_count:\n",
        "                u_dict = {}\n",
        "            else:\n",
        "                u_dict = transition_count[u]\n",
        "            \n",
        "            if v in u_dict:\n",
        "                u_dict[v] += 1\n",
        "            else:\n",
        "                u_dict[v] = 1\n",
        "            transition_count[u] = u_dict\n",
        "            u = v\n",
        "            \n",
        "        if len(line_split) != 2:\n",
        "            u_dict = transition_count[u]\n",
        "            v = stop\n",
        "            if v in u_dict:\n",
        "                u_dict[v] += 1\n",
        "            else:\n",
        "                u_dict[v] = 1\n",
        "            transition_count[u] = u_dict\n",
        "            \n",
        "            u = start\n",
        "            \n",
        "            \n",
        "    return transition_count\n",
        "\n",
        "def transition_para(transition_count, u, v):\n",
        "    if u not in transition_count:\n",
        "        a = 0\n",
        "        \n",
        "    else:\n",
        "        u_dict = transition_count[u]\n",
        "    \n",
        "        a = u_dict.get(v, 0)\n",
        "    \n",
        "        b = sum(u_dict.values())\n",
        "        \n",
        "    \n",
        "    return a/b\n",
        "\n",
        "def viterbi_forward(N, emissions, transitions, words, sentence):\n",
        "    n = len(sentence)\n",
        "    smallest = -9999999\n",
        "\n",
        "    states = list(transitions.keys())\n",
        "    states.remove(\"START\")\n",
        "\n",
        "    scores = {}\n",
        "    scores[0] = {}\n",
        "\n",
        "    for v in states:\n",
        "        # Transition Probability\n",
        "        transition_fraction = transition_para(transitions, \"START\", v)\n",
        "        if transition_fraction != 0:\n",
        "            trans = math.log(transition_fraction)\n",
        "        else:\n",
        "            trans = smallest\n",
        "        \n",
        "        if sentence[0] not in words:\n",
        "            token3 = \"#UNK#\"\n",
        "        else:\n",
        "            token3 = sentence[0]\n",
        "\n",
        "        # Emission Probability\n",
        "        if ((token3 in emissions[v]) or (token3 == \"#UNK#\")): \n",
        "            emmision_fraction = est_emission_param(emissions, token3, v)\n",
        "            emission = math.log(emmision_fraction)\n",
        "        else:\n",
        "            emission = smallest\n",
        "        \n",
        "        start = trans + emission\n",
        "        scores[0][v] = (\"START\", start)\n",
        "\n",
        "    copyscores = copy.deepcopy(scores)\n",
        "    \n",
        "    # State 1 to n\n",
        "    for i in range(1, n):\n",
        "        scores[i] = {}\n",
        "        copyscores[i] = {}\n",
        "        for v in states:\n",
        "            findmax = []\n",
        "            for u in states:\n",
        "                # Transition Probability\n",
        "                transition_fraction = transition_para(transitions, u, v)\n",
        "                if transition_fraction != 0:\n",
        "                    trans = math.log(transition_fraction)\n",
        "                else:\n",
        "                    trans = smallest\n",
        "                if sentence[i] not in words:\n",
        "                    v_v = \"#UNK#\"\n",
        "                else:\n",
        "                    v_v = sentence[i]\n",
        "\n",
        "                # Emission Probability\n",
        "                if ((v_v in emissions[v]) or (v_v == \"#UNK#\")): \n",
        "                    emmision_fraction = est_emission_param(emissions, v_v, v)\n",
        "                    emission = math.log(emmision_fraction)\n",
        "                else:\n",
        "                    emission = smallest\n",
        "              \n",
        "                if i == 1 :\n",
        "                  currentscore = scores[i-1][u][1] + trans + emission\n",
        "                  findmax.append(currentscore)\n",
        "                else:\n",
        "                    #two nested for loops\n",
        "                    # current score = [1st, 2nd , 3rd, 4th and 5th]\n",
        "                  currentscores = [[scores[i-1][u][m][1] for m in range(N)][j] + trans + emission for j in range(N)]\n",
        "                  for score in currentscores:\n",
        "                    findmax.append(score)\n",
        "            ans = [] \n",
        "            state_ans = []\n",
        "            copyfindmax = copy.deepcopy(findmax)\n",
        "            for m in range(N):\n",
        "                ans.append(max(copyfindmax))\n",
        "                # N=1 O or B postive, N=2 O, Positve, B neutral\n",
        "                state_ans.append(states[findmax.index(ans[m]) // N])\n",
        "                # print(\"NUMBER:::::\", N)\n",
        "                # print(\"THIS IS ANSWER::::::    \", ans[m])\n",
        "                # print(\"PRINT INDEX       \", findmax.index(ans[m] // N) )\n",
        "                copyfindmax[findmax.index(ans[m])] = -999999999.999\n",
        "            scores[i][v] = tuple((state_ans[m], ans[m]) for m in range(N))\n",
        "            \n",
        "\n",
        "    # STOP STATE\n",
        "    scores[n] = {}\n",
        "    copyscores[n] = {}\n",
        "    stopmax = []\n",
        "\n",
        "    for u in states:\n",
        "        # Transition Probability\n",
        "        transition_fraction = transition_para(transitions, u, \"STOP\")\n",
        "        if transition_fraction != 0:\n",
        "            trans = math.log(transition_fraction)\n",
        "        else:\n",
        "            trans = smallest\n",
        "\n",
        "        if(type(scores[n-1][u][0])==tuple):\n",
        "            stopscore = [[scores[n-1][u][m][1] for m in range(N)][j] + trans + emission for j in range(N)]\n",
        "        else:\n",
        "            t=scores[n-1][u]\n",
        "            stopscore = [t[1]+ trans + emission]    \n",
        "            \n",
        "        for score in stopscore:\n",
        "            stopmax.append(score)\n",
        "            \n",
        "\n",
        "    stop = []\n",
        "    state_ans = []\n",
        "    copystopmax = copy.deepcopy(stopmax)\n",
        "    for i in range(N):\n",
        "        stop.append(max(copystopmax))\n",
        "        state_ans.append(states[stopmax.index(stop[i]) // N])\n",
        "        copystopmax[stopmax.index(stop[i])] = -999999999.999\n",
        "    scores[n][u] = tuple((state_ans[m], stop[m]) for m in range(N))\n",
        "    \n",
        "      \n",
        "    N_bestPaths = []\n",
        "    lasts = [] \n",
        "    for i in range(N):\n",
        "      path = [\"STOP\"]\n",
        "      last = list(scores[n].values())[0][i][0]\n",
        "      lasts.append(last)\n",
        "      path.insert(0, last)\n",
        "      N_bestPaths.append(path)\n",
        "    \n",
        "    for i in range(N):\n",
        "        for k in range(n-1, -1, -1):\n",
        "            if k == 0:\n",
        "                last = scores[k][N_bestPaths[i][0]][0] \n",
        "            else:\n",
        "                last = scores[k][N_bestPaths[i][0]][0][0]\n",
        "            N_bestPaths[i].insert(0, last)\n",
        "    \n",
        "    \n",
        "    return N_bestPaths[N-1]\n",
        "\n",
        "def write_output(filepath, lines, all_pred_tags):\n",
        "\tprint(\"Writing output..\")\n",
        "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
        "\t\tfor j in range(len(lines)):\n",
        "\t\t\tword = lines[j].strip()\n",
        "\t\t\tif word != \"\\n\":\n",
        "\t\t\t\t# print(word)\n",
        "\t\t\t\ttag = all_pred_tags[j]\n",
        "\t\t\t\t# print(tag)\n",
        "\t\t\t\tif(tag != \"\\n\"):\n",
        "\t\t\t\t\tf.write(word + \" \" + tag)\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\n",
        "\tprint(\"Output successfully written!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    root_dir = \"./\"\n",
        "\n",
        "    datasets = [\"ES\", \"RU\"]\n",
        "\n",
        "    for dataset in datasets:\n",
        "\n",
        "        train_path = root_dir + \"{}/train\".format(dataset)\n",
        "        evaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
        "        \n",
        "        # training\n",
        "        transition_count = transition(train_path)\n",
        "\n",
        "        tokens3, emission_count = train(train_path)\n",
        "        \n",
        "        # evaluation\n",
        "        with open(evaluation_path, \"r\", encoding='utf8') as f:\n",
        "            lines = f.readlines()\n",
        "        sentence = []\n",
        "        \n",
        "        all_pred_tags = []\n",
        "        print(dataset)\n",
        "        N = 5\n",
        "        for line in lines:    \n",
        "            if line != \"\\n\":    \n",
        "                line = line.strip()\n",
        "                sentence.append(line)\n",
        "            else:\n",
        "                # print(emission_count)\n",
        "                sentence_prediction = viterbi_forward(N, emission_count, transition_count, tokens3, sentence)\n",
        "                sentence_prediction.remove(\"START\")\n",
        "                sentence_prediction.remove(\"STOP\")\n",
        "                # print(sentence_prediction)\n",
        "                all_pred_tags = all_pred_tags + sentence_prediction\n",
        "                all_pred_tags = all_pred_tags + [\"\\n\"]\n",
        "                # print(all_pred_tags)\n",
        "                sentence = []\n",
        "        # print(\"length of lines:\", len(lines),\"lenth of all tags:\", len(all_pred_tags))\n",
        "        assert len(lines) == len(all_pred_tags)\n",
        "        print(\"All words have a tag. Proceeding..\")\n",
        "\n",
        "        output_path = root_dir + \"{}/dev.p3.out\".format(dataset)\n",
        "        # print(all_pred_tags)\n",
        "        write_output(output_path, lines, all_pred_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For dataset ES:\n",
            "ES\n",
            "All words have a tag. Proceeding..\n",
            "Writing output..\n",
            "Output successfully written!\n",
            "For dataset RU:\n",
            "RU\n",
            "All words have a tag. Proceeding..\n",
            "Writing output..\n",
            "Output successfully written!\n"
          ]
        }
      ],
      "source": [
        "def train(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        " \n",
        "    tokens = set()\n",
        "    \n",
        "    emission_count = {}\n",
        "    \n",
        "    for line in lines:\n",
        "        line_split = line.strip().rsplit(\" \", 1)\n",
        "\n",
        "        if len(line_split) == 2:\n",
        "            token = line_split[0]\n",
        "            tag = line_split[1]        \n",
        "            tokens.add(token)\n",
        "    \n",
        "            if tag in emission_count:\n",
        "                nested_tag_dict = emission_count[tag]\n",
        "            else:\n",
        "                nested_tag_dict = {}\n",
        "                \n",
        "            if token in nested_tag_dict:\n",
        "                nested_tag_dict[token] = nested_tag_dict[token] + 1\n",
        "            else:\n",
        "                nested_tag_dict[token] = 1\n",
        "            \n",
        "            emission_count[tag] = nested_tag_dict\n",
        "    # print(\"THIS IS TOKENS::::::::::::::::::::::::\",tokens)\n",
        "    # print(\"THIS IS CCOUNT::::::::::::::::::::::::\",emission_count)\n",
        "    return tokens, emission_count\n",
        "#Laplace smoothing\n",
        "\n",
        "\n",
        "# def est_emission_param(emission_count, token, tag, k=1):\n",
        "# \ttag_dict = emission_count[tag]\n",
        "\n",
        "# \tif token != \"#UNK#\":\n",
        "# \t\ta = tag_dict.get(token, 0)\n",
        "# \telse:\n",
        "# \t\ta = k \n",
        "# \tsumz = 0\n",
        "# \tfor z in tag_dict.values():\n",
        "# \t\tsumz += z + 1\t\n",
        "# \t# b = sum(tag_dict.values()) + k\n",
        "\n",
        "# \treturn a + 1 / (sumz + k)\n",
        "\n",
        "\n",
        "# Bruteforce for the best K value \n",
        "def est_emission_param(emission_count, token, tag, k = 4):\n",
        "    tag_dict = emission_count[tag]\n",
        "    \n",
        "    tag_dict = emission_count[tag]\n",
        "\n",
        "    if token != \"#UNK#\":\n",
        "      # print(list(emission_count.keys())[0])\n",
        "      # print(emission_count[list(emission_count.keys())[0]])\n",
        "      # print(([i for i in list(emission_count.keys())]))\n",
        "      # print(sum([sum(list(emission_count[i].values())) for i in list(emission_count.keys())]))\n",
        "      \n",
        "      a = tag_dict.get(token, 0)\n",
        "    else:\n",
        "      return sum(list(emission_count[tag].values())) / sum([sum(list(emission_count[i].values())) for i in list(emission_count.keys())])\n",
        "      # a = k \n",
        "    b = sum(tag_dict.values()) + k\n",
        "    \n",
        "\n",
        "    return a / b\n",
        "\n",
        "def transition(filepath):\n",
        "  with open(filepath, 'r', encoding=\"utf8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  start = 'START'\n",
        "  stop = 'STOP'\n",
        "\n",
        "  u = start\n",
        "  v = start \n",
        "  \n",
        "  \n",
        "  transition_count = {}\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  for line in lines:\n",
        "    line_split = line.strip().rsplit(\" \", 1)\n",
        "    \n",
        "    # case 1: word line\n",
        "    if len(line_split) == 2:\n",
        "      token = line_split[0]\n",
        "      w = line_split[1]\n",
        "\n",
        "      if u not in transition_count:\n",
        "        transition_count[u] = {}\n",
        "      \n",
        "      if v not in transition_count[u]:\n",
        "        v_dict = {}\n",
        "      else:\n",
        "        v_dict = transition_count[u][v]\n",
        "          \n",
        "      if w in v_dict:\n",
        "        v_dict[w] += 1\n",
        "      else:\n",
        "        v_dict[w] = 1\n",
        "\n",
        "      transition_count[u][v] = v_dict\n",
        "\n",
        "      u = v\n",
        "      v = w\n",
        "\n",
        "    if len(line_split) != 2:\n",
        "\n",
        "      if u not in transition_count:\n",
        "        transition_count[u] = {}\n",
        "      \n",
        "      if v not in transition_count[u]:\n",
        "        v_dict = {}\n",
        "      else:\n",
        "        v_dict = transition_count[u][v]\n",
        "      w = stop\n",
        "\n",
        "      if w in v_dict:\n",
        "        v_dict[w] += 1\n",
        "      else:\n",
        "        v_dict[w] = 1\n",
        "\n",
        "\n",
        "      transition_count[u][v] = v_dict \n",
        "\n",
        "      u = start\n",
        "      v = start\n",
        "\n",
        "  return transition_count\n",
        "\n",
        "\n",
        "\n",
        "def transition_param(transition_count, u, v, w):\n",
        "  \n",
        "  if u not in transition_count:\n",
        "    a = 0\n",
        "    b = 1\n",
        "  elif v not in transition_count[u]:\n",
        "    a = 0\n",
        "    b = 1\n",
        "  else:\n",
        "    v_dict = transition_count[u][v]\n",
        "\n",
        "    a = v_dict.get(w, 0)\n",
        "\n",
        "    b = sum(v_dict.values())\n",
        "    \n",
        "  return a / b\n",
        "\n",
        "def viterbi_forward(emission_count, transition_count, tokens, sentence):\n",
        "  n = len(sentence)\n",
        "  smallest = -999999\n",
        "  TOKEN = \"#UNK#\"\n",
        "\n",
        "  states = list(transition_count.keys())\n",
        "  states.remove('START')\n",
        "  \n",
        "  scores = {}\n",
        "\n",
        "\n",
        "\n",
        "  scores[0] = {}\n",
        "\n",
        "  for w in states:\n",
        "    trans_frac = transition_param(transition_count, 'START', 'START', w)\n",
        "\n",
        "    if trans_frac != 0:\n",
        "      trans = math.log(trans_frac)\n",
        "    else:\n",
        "      trans = smallest\n",
        "    \n",
        "    if sentence[0] not in tokens:\n",
        "      token = TOKEN\n",
        "    else:\n",
        "      token = sentence[0]\n",
        "\n",
        "    if ((token in emission_count[w]) or (token == TOKEN)):\n",
        "      emis_frac = est_emission_param(emission_count, token, w)\n",
        "      emission = math.log(emis_frac)\n",
        "    else:\n",
        "      emission = smallest\n",
        "\n",
        "    start_score = trans + emission\n",
        "    scores[0][w] = {}\n",
        "    scores[0][w]['START'] = {\"START\": start_score}\n",
        "\n",
        "  if n == 1:\n",
        "    scores[n] = {}\n",
        "    scores[n][\"STOP\"] = {}\n",
        "\n",
        "    scores[n][\"STOP\"][\"START\"] = {}\n",
        "    for v in states:\n",
        "      trans_frac = transition_param(transition_count, \"START\", v, \"STOP\")\n",
        "      if trans_frac != 0:\n",
        "        trans = math.log(trans_frac)\n",
        "      else:\n",
        "        trans = smallest\n",
        "        \n",
        "      best_v = scores[0][v][\"START\"][\"START\"]\n",
        "      current_stop_score = best_v + trans\n",
        "      scores[n][\"STOP\"][\"START\"][v] = current_stop_score\n",
        "    \n",
        "    path = [\"STOP\"]\n",
        "    stop_lst = []\n",
        "\n",
        "    for v in scores[n][\"STOP\"][\"START\"]:\n",
        "      stop_lst.append((v, scores[n][\"STOP\"][\"START\"][v]))\n",
        "    \n",
        "    max_state_v_for_start_state = max(stop_lst, key=itemgetter(1))\n",
        "    \n",
        "    path.insert(0, max_state_v_for_start_state[0])\n",
        "    path.insert(0, \"START\")\n",
        "\n",
        "    return path\n",
        "\n",
        "  else:\n",
        "    scores[1] = {}\n",
        "\n",
        "    for w in states:\n",
        "      scores[1][w] = {}\n",
        "      scores[1][w][\"START\"] = {}\n",
        "      for v in states:\n",
        "        trans_frac = transition_param(transition_count, 'START', v, w)\n",
        "\n",
        "        if trans_frac != 0:\n",
        "          trans = math.log(trans_frac)\n",
        "        else:\n",
        "          trans = smallest\n",
        "        \n",
        "        if sentence[1] not in tokens:\n",
        "          token = TOKEN\n",
        "        else:\n",
        "          token = sentence[1]\n",
        "\n",
        "        if ((token in emission_count[w]) or (token == TOKEN)):\n",
        "          emis_frac = est_emission_param(emission_count, token, w)\n",
        "          emission = math.log(emis_frac)\n",
        "        else:\n",
        "          emission = smallest\n",
        "\n",
        "        current_score = scores[0][v]['START']['START'] + trans + emission\n",
        "        scores[1][w]['START'][v] = current_score\n",
        "\n",
        "  if n == 2:\n",
        "    scores[2] = {}\n",
        "    scores[2][\"STOP\"] = {}\n",
        "\n",
        "    for u in states:\n",
        "      scores[n][\"STOP\"][u] = {}\n",
        "      for v in states:\n",
        "        trans_frac = transition_param(transition_count, u, v, 'STOP')\n",
        "        if trans_frac != 0:\n",
        "          trans = math.log(trans_frac)\n",
        "        else:\n",
        "          trans = smallest\n",
        "\n",
        "        state_v_arr = []\n",
        "        \n",
        "        for old_u in scores[n-1][v]:\n",
        "          state_v_arr.append(scores[n-1][v][old_u][u])\n",
        "        \n",
        "        best_v = max(state_v_arr)\n",
        "        current_stop_score = best_v + trans\n",
        "        scores[n][\"STOP\"][u][v] = current_stop_score\n",
        "\n",
        "    # Backtracking path\n",
        "    path = [\"STOP\"]\n",
        "    stop_lst = []\n",
        "\n",
        "    for u in scores[n][\"STOP\"]:\n",
        "      state_u_lst = []\n",
        "      for v in scores[n][\"STOP\"][u]:\n",
        "        state_u_lst.append((v, scores[n][\"STOP\"][u][v]))\n",
        "      \n",
        "        \n",
        "      max_state_v_for_state_u = max(state_u_lst, key=itemgetter(1))\n",
        "      \n",
        "      max_tuple = (u, max_state_v_for_state_u[0], max_state_v_for_state_u[1])\n",
        "      stop_lst.append(max_tuple)\n",
        "\n",
        "    max_stop_tuple = max(stop_lst, key=itemgetter(2))\n",
        "    \n",
        "    path.insert(0, max_stop_tuple[1])\n",
        "    path.insert(0, max_stop_tuple[0])\n",
        "\n",
        "    prev = -2\n",
        "\n",
        "    for k in range(n-1, 0, -1):\n",
        "      u = scores[k][path[prev]] \n",
        "      state_v_lst = []\n",
        "      \n",
        "      for i in u.keys():\n",
        "        if path[prev-1] in u[i]:\n",
        "          state_v_lst.append((i, u[i][path[prev-1]]))\n",
        "      \n",
        "      max_score = max(state_v_lst, key=itemgetter(1))\n",
        "      \n",
        "      prev = prev - 1\n",
        "      \n",
        "      path.insert(0, max_score[0])\n",
        "  \n",
        "  elif n > 2:\n",
        "    scores[2] = {}\n",
        "    for w in states:\n",
        "      scores[2][w] = {}\n",
        "      for u in states:\n",
        "        scores[2][w][u] = {}\n",
        "        for v in states:\n",
        "          # Transition Probability\n",
        "          trans_frac = transition_param(transition_count, u, v, w)\n",
        "\n",
        "          if trans_frac != 0:\n",
        "            trans = math.log(trans_frac)\n",
        "          else:\n",
        "            trans = smallest\n",
        "          \n",
        "          if sentence[2] not in tokens:\n",
        "            token = TOKEN\n",
        "          else:\n",
        "            token = sentence[2]\n",
        "\n",
        "          if ((token in emission_count[w]) or (token == TOKEN)):\n",
        "            emis_frac = est_emission_param(emission_count, token, w)\n",
        "            emission = math.log(emis_frac)\n",
        "          else:\n",
        "            emission = smallest\n",
        "\n",
        "          current_score = scores[1][v]['START'][u] + trans + emission\n",
        "          scores[2][w][u][v] = current_score\n",
        "\n",
        "\n",
        "    for i in range(3,n):\n",
        "      scores[i] = {}\n",
        "      for w in states:\n",
        "        scores[i][w] = {}\n",
        "        for u in states:\n",
        "          scores[i][w][u] = {}\n",
        "          for v in states:\n",
        "            # Transition Probability\n",
        "            trans_frac = transition_param(transition_count, u, v, w)\n",
        "            if trans_frac != 0:\n",
        "              trans = math.log(trans_frac)\n",
        "            else:\n",
        "              trans = smallest\n",
        "\n",
        "            if sentence[i] not in tokens:\n",
        "              token = TOKEN\n",
        "            else:\n",
        "              token = sentence[i]\n",
        "\n",
        "            # Emission Probability\n",
        "            if ((token in emission_count[w]) or (token == TOKEN)):\n",
        "              emis_frac = est_emission_param(emission_count, token, w)\n",
        "              emission = math.log(emis_frac)\n",
        "            else:\n",
        "              emission = smallest\n",
        "\n",
        "            state_v_arr = []\n",
        "            for old_u in scores[i-1][v]:\n",
        "                state_v_arr.append(scores[i-1][v][old_u][u])\n",
        "            best_v = max(state_v_arr)\n",
        "\n",
        "            current_score = best_v + trans + emission\n",
        "            scores[i][w][u][v] = current_score\n",
        "\n",
        "    scores[n] = {}\n",
        "    scores[n][\"STOP\"] = {}\n",
        "\n",
        "    for u in states:\n",
        "      scores[n][\"STOP\"][u] = {}\n",
        "      for v in states:\n",
        "        # Transition Probability\n",
        "        trans_frac = transition_param(transition_count, u, v, 'STOP')\n",
        "        if trans_frac != 0:\n",
        "          trans = math.log(trans_frac)\n",
        "        else:\n",
        "          trans = smallest\n",
        "        \n",
        "        state_v_arr = []\n",
        "        for old_u in scores[n-1][v]:\n",
        "          state_v_arr.append(scores[n-1][v][old_u][u])\n",
        "        \n",
        "        best_v = max(state_v_arr)\n",
        "        current_stop_score = best_v + trans\n",
        "        scores[n][\"STOP\"][u][v] = current_stop_score\n",
        "\n",
        "\n",
        "    # Backtracking path\n",
        "    path = [\"STOP\"]\n",
        "    stop_lst = []\n",
        "\n",
        "    for u in scores[n][\"STOP\"]:\n",
        "      state_u_lst = []\n",
        "      for v in scores[n][\"STOP\"][u]:\n",
        "        state_u_lst.append((v, scores[n][\"STOP\"][u][v]))\n",
        "      \n",
        "      max_state_v_for_state_u = max(state_u_lst, key=itemgetter(1))\n",
        "      \n",
        "      max_tuple = (u, max_state_v_for_state_u[0], max_state_v_for_state_u[1])\n",
        "      stop_lst.append(max_tuple)\n",
        "    \n",
        "    max_stop_tuple = max(stop_lst, key=itemgetter(2))\n",
        "    \n",
        "    path.insert(0, max_stop_tuple[1])\n",
        "    path.insert(0, max_stop_tuple[0])\n",
        "\n",
        "    prev = -2\n",
        "\n",
        "    for k in range(n-1, 0, -1):\n",
        "      u = scores[k][path[prev]]\n",
        "      state_v_lst = []\n",
        "\n",
        "\n",
        "      for i in u.keys():\n",
        "        if path[prev-1] in u[i]:\n",
        "          state_v_lst.append((i, u[i][path[prev-1]]))\n",
        "      \n",
        "\n",
        "      max_score = max(state_v_lst, key=itemgetter(1))\n",
        "      \n",
        "      prev = prev - 1\n",
        "      \n",
        "\n",
        "      path.insert(0, max_score[0])\n",
        "\n",
        "\n",
        "  return path\n",
        "\n",
        "\n",
        "\n",
        "def write_output(filepath, lines, all_pred_tags):\n",
        "\tprint(\"Writing output..\")\n",
        "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
        "\t\tfor j in range(len(lines)):\n",
        "\t\t\tword = lines[j].strip()\n",
        "\t\t\tif word != \"\\n\":\n",
        "\t\t\t\t# print(word)\n",
        "\t\t\t\ttag = all_pred_tags[j]\n",
        "\t\t\t\t# print(tag)\n",
        "\t\t\t\tif(tag != \"\\n\"):\n",
        "\t\t\t\t\tf.write(word + \" \" + tag)\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\tprint(\"Output successfully written!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\troot_dir = \"./\"\n",
        "\tdatasets = [\"ES\", \"RU\"]\n",
        "\n",
        "\tfor dataset in datasets:\n",
        "\t\tprint(\"For dataset {}:\".format(dataset))\n",
        "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
        "\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
        "\n",
        "\t\t# Train\n",
        "\t\ttransition_count = transition(train_path)\n",
        "\t\ttokens, emission_count = train(train_path)\n",
        "\n",
        "\t\twith open(evaluation_path, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "\t\t\tlines = f.readlines()\n",
        "\n",
        "\t\tsentence = []\n",
        "\t\tall_pred_tags = []\n",
        "\t\tprint(dataset)\n",
        "\n",
        "\t\tfor line in lines:    \n",
        "\t\t\tif line != \"\\n\":    \n",
        "\t\t\t\tline = line.strip()\n",
        "\t\t\t\tsentence.append(line)\n",
        "\t\t\telse:\n",
        "\t\t\t\t# print(emission_count)\n",
        "\t\t\t\tsentence_prediction = viterbi_forward(emission_count, transition_count, tokens, sentence)\n",
        "\t\t\t\tsentence_prediction.remove(\"START\")\n",
        "\t\t\t\tsentence_prediction.remove(\"STOP\")\n",
        "\t\t\t\t# print(sentence_prediction)\n",
        "\t\t\t\tall_pred_tags = all_pred_tags + sentence_prediction\n",
        "\t\t\t\tall_pred_tags = all_pred_tags + [\"\\n\"]\n",
        "\t\t\t\t# print(all_pred_tags)\n",
        "\t\t\t\tsentence = []\n",
        "\t\t# print(\"length of lines:\", len(lines),\"lenth of all tags:\", len(all_pred_tags))\n",
        "\t\tassert len(lines) == len(all_pred_tags)\n",
        "\t\tprint(\"All words have a tag. Proceeding..\")\n",
        "\n",
        "\t\toutput_path = root_dir + \"{}/dev.p4.out\".format(dataset)\n",
        "\t\t# print(all_pred_tags)\n",
        "\t\twrite_output(output_path, lines, all_pred_tags)\n",
        "\t\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [-test]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"36f012bd-7a96-4c65-9366-4b81447d01d9\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=C:\\Users\\HGarg\\AppData\\Local\\Temp\\tmp-21068xHa3Xbhg771b.json\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "2",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "#Libraries\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import copy\n",
        "from operator import *\n",
        "import argparse\n",
        "\n",
        "def train(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        " \n",
        "    tokens = set()\n",
        "    \n",
        "    emission_count = {}\n",
        "    \n",
        "    for line in lines:\n",
        "        line_split = line.strip().rsplit(\" \", 1)\n",
        "\n",
        "        if len(line_split) == 2:\n",
        "            token = line_split[0]\n",
        "            tag = line_split[1]        \n",
        "            tokens.add(token)\n",
        "    \n",
        "            if tag in emission_count:\n",
        "                nested_tag_dict = emission_count[tag]\n",
        "            else:\n",
        "                nested_tag_dict = {}\n",
        "            # print(nested_tag_dict)\n",
        "                \n",
        "            if token in nested_tag_dict:\n",
        "                nested_tag_dict[token] = nested_tag_dict[token] + 1\n",
        "            else:\n",
        "                nested_tag_dict[token] = 1\n",
        "            \n",
        "            emission_count[tag] = nested_tag_dict\n",
        "    # print(nested_tag_dict)\n",
        "    print(\"THIS IS TOKENS::::::::::::::::::::::::\",tokens)\n",
        "    print(\"THIS IS CCOUNT::::::::::::::::::::::::\",emission_count)\n",
        "    return tokens, emission_count\n",
        "    \n",
        "#Laplace smoothing\n",
        "\n",
        "\n",
        "# def est_emission_param(emission_count, token, tag, k=1):\n",
        "# \ttag_dict = emission_count[tag]\n",
        "\n",
        "# \tif token != \"#UNK#\":\n",
        "# \t\ta = tag_dict.get(token, 0)\n",
        "# \telse:\n",
        "# \t\ta = k \n",
        "# \tsumz = 0\n",
        "# \tfor z in tag_dict.values():\n",
        "# \t\tsumz += z + 1\t\n",
        "# \t# b = sum(tag_dict.values()) + k\n",
        "\n",
        "# \treturn a + 1 / (sumz + k)\n",
        "\n",
        "\n",
        "# Bruteforce for the best K value \n",
        "# Distribuition of states\n",
        "def est_emission_param(emission_count, token, tag, k = 4):\n",
        "    tag_dict = emission_count[tag]\n",
        "    \n",
        "    tag_dict = emission_count[tag]\n",
        "\n",
        "    if token != \"#UNK#\":\n",
        "      # print(list(emission_count.keys())[0])\n",
        "      # print(emission_count[list(emission_count.keys())[0]])\n",
        "      # print(([i for i in list(emission_count.keys())]))\n",
        "      # print(sum([sum(list(emission_count[i].values())) for i in list(emission_count.keys())]))\n",
        "      \n",
        "      a = tag_dict.get(token, 0)\n",
        "    else:\n",
        "      return sum(list(emission_count[tag].values())) / sum([sum(list(emission_count[i].values())) for i in list(emission_count.keys())])\n",
        "      # a = k \n",
        "    b = sum(tag_dict.values()) + k\n",
        "    \n",
        "\n",
        "    return a / b\n",
        "\n",
        "def transition(filepath):\n",
        "  with open(filepath, 'r', encoding=\"utf8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  start = 'START'\n",
        "  stop = 'STOP'\n",
        "\n",
        "  u = start\n",
        "  v = start \n",
        "  \n",
        "  \n",
        "  transition_count = {}\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  for line in lines:\n",
        "    line_split = line.strip().rsplit(\" \", 1)\n",
        "    \n",
        "    # case 1: word line\n",
        "    if len(line_split) == 2:\n",
        "      token = line_split[0]\n",
        "      w = line_split[1]\n",
        "\n",
        "      if u not in transition_count:\n",
        "        transition_count[u] = {}\n",
        "      \n",
        "      if v not in transition_count[u]:\n",
        "        v_dict = {}\n",
        "      else:\n",
        "        v_dict = transition_count[u][v]\n",
        "          \n",
        "      if w in v_dict:\n",
        "        v_dict[w] += 1\n",
        "      else:\n",
        "        v_dict[w] = 1\n",
        "\n",
        "      transition_count[u][v] = v_dict\n",
        "\n",
        "      u = v\n",
        "      v = w\n",
        "\n",
        "    if len(line_split) != 2:\n",
        "\n",
        "      if u not in transition_count:\n",
        "        transition_count[u] = {}\n",
        "      \n",
        "      if v not in transition_count[u]:\n",
        "        v_dict = {}\n",
        "      else:\n",
        "        v_dict = transition_count[u][v]\n",
        "      w = stop\n",
        "\n",
        "      if w in v_dict:\n",
        "        v_dict[w] += 1\n",
        "      else:\n",
        "        v_dict[w] = 1\n",
        "\n",
        "\n",
        "      transition_count[u][v] = v_dict \n",
        "\n",
        "      u = start\n",
        "      v = start\n",
        "\n",
        "  return transition_count\n",
        "\n",
        "\n",
        "\n",
        "def transition_param(transition_count, u, v, w):\n",
        "  \n",
        "  if u not in transition_count:\n",
        "    a = 0\n",
        "    b = 1\n",
        "  elif v not in transition_count[u]:\n",
        "    a = 0\n",
        "    b = 1\n",
        "  else:\n",
        "    v_dict = transition_count[u][v]\n",
        "\n",
        "    a = v_dict.get(w, 0)\n",
        "\n",
        "    b = sum(v_dict.values())\n",
        "    \n",
        "  return a / b\n",
        "\n",
        "def viterbi_forward(emission_count, transition_count, tokens, sentence):\n",
        "  n = len(sentence)\n",
        "  smallest = -999999\n",
        "  TOKEN = \"#UNK#\"\n",
        "\n",
        "  states = list(transition_count.keys())\n",
        "  states.remove('START')\n",
        "  \n",
        "  scores = {}\n",
        "\n",
        "\n",
        "\n",
        "  scores[0] = {}\n",
        "\n",
        "  for w in states:\n",
        "    trans_frac = transition_param(transition_count, 'START', 'START', w)\n",
        "\n",
        "    if trans_frac != 0:\n",
        "      trans = math.log(trans_frac)\n",
        "    else:\n",
        "      trans = smallest\n",
        "    \n",
        "    if sentence[0] not in tokens:\n",
        "      token = TOKEN\n",
        "    else:\n",
        "      token = sentence[0]\n",
        "\n",
        "    if ((token in emission_count[w]) or (token == TOKEN)):\n",
        "      emis_frac = est_emission_param(emission_count, token, w)\n",
        "      emission = math.log(emis_frac)\n",
        "    else:\n",
        "      emission = smallest\n",
        "\n",
        "    start_score = trans + emission\n",
        "    scores[0][w] = {}\n",
        "    scores[0][w]['START'] = {\"START\": start_score}\n",
        "\n",
        "  if n == 1:\n",
        "    scores[n] = {}\n",
        "    scores[n][\"STOP\"] = {}\n",
        "\n",
        "    scores[n][\"STOP\"][\"START\"] = {}\n",
        "    for v in states:\n",
        "      trans_frac = transition_param(transition_count, \"START\", v, \"STOP\")\n",
        "      if trans_frac != 0:\n",
        "        trans = math.log(trans_frac)\n",
        "      else:\n",
        "        trans = smallest\n",
        "        \n",
        "      best_v = scores[0][v][\"START\"][\"START\"]\n",
        "      current_stop_score = best_v + trans\n",
        "      scores[n][\"STOP\"][\"START\"][v] = current_stop_score\n",
        "    \n",
        "    path = [\"STOP\"]\n",
        "    stop_lst = []\n",
        "\n",
        "    for v in scores[n][\"STOP\"][\"START\"]:\n",
        "      stop_lst.append((v, scores[n][\"STOP\"][\"START\"][v]))\n",
        "    \n",
        "    max_state_v_for_start_state = max(stop_lst, key=itemgetter(1))\n",
        "    \n",
        "    path.insert(0, max_state_v_for_start_state[0])\n",
        "    path.insert(0, \"START\")\n",
        "\n",
        "    return path\n",
        "\n",
        "  else:\n",
        "    scores[1] = {}\n",
        "\n",
        "    for w in states:\n",
        "      scores[1][w] = {}\n",
        "      scores[1][w][\"START\"] = {}\n",
        "      for v in states:\n",
        "        trans_frac = transition_param(transition_count, 'START', v, w)\n",
        "\n",
        "        if trans_frac != 0:\n",
        "          trans = math.log(trans_frac)\n",
        "        else:\n",
        "          trans = smallest\n",
        "        \n",
        "        if sentence[1] not in tokens:\n",
        "          token = TOKEN\n",
        "        else:\n",
        "          token = sentence[1]\n",
        "\n",
        "        if ((token in emission_count[w]) or (token == TOKEN)):\n",
        "          emis_frac = est_emission_param(emission_count, token, w)\n",
        "          emission = math.log(emis_frac)\n",
        "        else:\n",
        "          emission = smallest\n",
        "\n",
        "        current_score = scores[0][v]['START']['START'] + trans + emission\n",
        "        scores[1][w]['START'][v] = current_score\n",
        "\n",
        "  if n == 2:\n",
        "    scores[2] = {}\n",
        "    scores[2][\"STOP\"] = {}\n",
        "\n",
        "    for u in states:\n",
        "      scores[n][\"STOP\"][u] = {}\n",
        "      for v in states:\n",
        "        trans_frac = transition_param(transition_count, u, v, 'STOP')\n",
        "        if trans_frac != 0:\n",
        "          trans = math.log(trans_frac)\n",
        "        else:\n",
        "          trans = smallest\n",
        "\n",
        "        state_v_arr = []\n",
        "        \n",
        "        for old_u in scores[n-1][v]:\n",
        "          state_v_arr.append(scores[n-1][v][old_u][u])\n",
        "        \n",
        "        best_v = max(state_v_arr)\n",
        "        current_stop_score = best_v + trans\n",
        "        scores[n][\"STOP\"][u][v] = current_stop_score\n",
        "\n",
        "    # Backtracking path\n",
        "    path = [\"STOP\"]\n",
        "    stop_lst = []\n",
        "\n",
        "    for u in scores[n][\"STOP\"]:\n",
        "      state_u_lst = []\n",
        "      for v in scores[n][\"STOP\"][u]:\n",
        "        state_u_lst.append((v, scores[n][\"STOP\"][u][v]))\n",
        "      \n",
        "        \n",
        "      max_state_v_for_state_u = max(state_u_lst, key=itemgetter(1))\n",
        "      \n",
        "      max_tuple = (u, max_state_v_for_state_u[0], max_state_v_for_state_u[1])\n",
        "      stop_lst.append(max_tuple)\n",
        "\n",
        "    max_stop_tuple = max(stop_lst, key=itemgetter(2))\n",
        "    \n",
        "    path.insert(0, max_stop_tuple[1])\n",
        "    path.insert(0, max_stop_tuple[0])\n",
        "\n",
        "    prev = -2\n",
        "\n",
        "    for k in range(n-1, 0, -1):\n",
        "      u = scores[k][path[prev]] \n",
        "      state_v_lst = []\n",
        "      \n",
        "      for i in u.keys():\n",
        "        if path[prev-1] in u[i]:\n",
        "          state_v_lst.append((i, u[i][path[prev-1]]))\n",
        "      \n",
        "      max_score = max(state_v_lst, key=itemgetter(1))\n",
        "      \n",
        "      prev = prev - 1\n",
        "      \n",
        "      path.insert(0, max_score[0])\n",
        "  \n",
        "  elif n > 2:\n",
        "    scores[2] = {}\n",
        "    for w in states:\n",
        "      scores[2][w] = {}\n",
        "      for u in states:\n",
        "        scores[2][w][u] = {}\n",
        "        for v in states:\n",
        "          # Transition Probability\n",
        "          trans_frac = transition_param(transition_count, u, v, w)\n",
        "\n",
        "          if trans_frac != 0:\n",
        "            trans = math.log(trans_frac)\n",
        "          else:\n",
        "            trans = smallest\n",
        "          \n",
        "          if sentence[2] not in tokens:\n",
        "            token = TOKEN\n",
        "          else:\n",
        "            token = sentence[2]\n",
        "\n",
        "          if ((token in emission_count[w]) or (token == TOKEN)):\n",
        "            emis_frac = est_emission_param(emission_count, token, w)\n",
        "            emission = math.log(emis_frac)\n",
        "          else:\n",
        "            emission = smallest\n",
        "\n",
        "          current_score = scores[1][v]['START'][u] + trans + emission\n",
        "          scores[2][w][u][v] = current_score\n",
        "\n",
        "\n",
        "    for i in range(3,n):\n",
        "      scores[i] = {}\n",
        "      for w in states:\n",
        "        scores[i][w] = {}\n",
        "        for u in states:\n",
        "          scores[i][w][u] = {}\n",
        "          for v in states:\n",
        "            # Transition Probability\n",
        "            trans_frac = transition_param(transition_count, u, v, w)\n",
        "            if trans_frac != 0:\n",
        "              trans = math.log(trans_frac)\n",
        "            else:\n",
        "              trans = smallest\n",
        "\n",
        "            if sentence[i] not in tokens:\n",
        "              token = TOKEN\n",
        "            else:\n",
        "              token = sentence[i]\n",
        "\n",
        "            # Emission Probability\n",
        "            if ((token in emission_count[w]) or (token == TOKEN)):\n",
        "              emis_frac = est_emission_param(emission_count, token, w)\n",
        "              emission = math.log(emis_frac)\n",
        "            else:\n",
        "              emission = smallest\n",
        "\n",
        "            state_v_arr = []\n",
        "            for old_u in scores[i-1][v]:\n",
        "                state_v_arr.append(scores[i-1][v][old_u][u])\n",
        "            best_v = max(state_v_arr)\n",
        "\n",
        "            current_score = best_v + trans + emission\n",
        "            scores[i][w][u][v] = current_score\n",
        "\n",
        "    scores[n] = {}\n",
        "    scores[n][\"STOP\"] = {}\n",
        "\n",
        "    for u in states:\n",
        "      scores[n][\"STOP\"][u] = {}\n",
        "      for v in states:\n",
        "        # Transition Probability\n",
        "        trans_frac = transition_param(transition_count, u, v, 'STOP')\n",
        "        if trans_frac != 0:\n",
        "          trans = math.log(trans_frac)\n",
        "        else:\n",
        "          trans = smallest\n",
        "        \n",
        "        state_v_arr = []\n",
        "        for old_u in scores[n-1][v]:\n",
        "          state_v_arr.append(scores[n-1][v][old_u][u])\n",
        "        \n",
        "        best_v = max(state_v_arr)\n",
        "        current_stop_score = best_v + trans\n",
        "        scores[n][\"STOP\"][u][v] = current_stop_score\n",
        "\n",
        "\n",
        "    # Backtracking path\n",
        "    path = [\"STOP\"]\n",
        "    stop_lst = []\n",
        "\n",
        "    for u in scores[n][\"STOP\"]:\n",
        "      state_u_lst = []\n",
        "      for v in scores[n][\"STOP\"][u]:\n",
        "        state_u_lst.append((v, scores[n][\"STOP\"][u][v]))\n",
        "      \n",
        "      max_state_v_for_state_u = max(state_u_lst, key=itemgetter(1))\n",
        "      \n",
        "      max_tuple = (u, max_state_v_for_state_u[0], max_state_v_for_state_u[1])\n",
        "      stop_lst.append(max_tuple)\n",
        "    \n",
        "    max_stop_tuple = max(stop_lst, key=itemgetter(2))\n",
        "    \n",
        "    path.insert(0, max_stop_tuple[1])\n",
        "    path.insert(0, max_stop_tuple[0])\n",
        "\n",
        "    prev = -2\n",
        "\n",
        "    for k in range(n-1, 0, -1):\n",
        "      u = scores[k][path[prev]]\n",
        "      state_v_lst = []\n",
        "\n",
        "\n",
        "      for i in u.keys():\n",
        "        if path[prev-1] in u[i]:\n",
        "          state_v_lst.append((i, u[i][path[prev-1]]))\n",
        "      \n",
        "\n",
        "      max_score = max(state_v_lst, key=itemgetter(1))\n",
        "      \n",
        "      prev = prev - 1\n",
        "      \n",
        "\n",
        "      path.insert(0, max_score[0])\n",
        "\n",
        "\n",
        "  return path\n",
        "\n",
        "\n",
        "\n",
        "def write_output(filepath, lines, all_pred_tags):\n",
        "\tprint(\"Writing output..\")\n",
        "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
        "\t\tfor j in range(len(lines)):\n",
        "\t\t\tword = lines[j].strip()\n",
        "\t\t\tif word != \"\\n\":\n",
        "\t\t\t\t# print(word)\n",
        "\t\t\t\ttag = all_pred_tags[j]\n",
        "\t\t\t\t# print(tag)\n",
        "\t\t\t\tif(tag != \"\\n\"):\n",
        "\t\t\t\t\tf.write(word + \" \" + tag)\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tf.write(\"\\n\")\n",
        "\tprint(\"Output successfully written!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tparser = argparse.ArgumentParser()\n",
        "\tparser.add_argument(\"-test\", action='store_true')\n",
        "\targ = parser.parse_args()\n",
        "\n",
        "\troot_dir = \"./\"\n",
        "\tdatasets = [\"ES\", \"RU\"]\n",
        "\n",
        "\tfor dataset in datasets:\n",
        "\t\tprint(\"For dataset {}:\".format(dataset)) \n",
        "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
        "\t\tif arg.test:\n",
        "\t\t\tevaluation_path = root_dir + \"{}/test.in\".format(dataset + \"-test\")\n",
        "\t\telse:\n",
        "\t\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
        "\n",
        "\t\t# Train\n",
        "\t\ttransition_count = transition(train_path)\n",
        "\t\ttokens, emission_count = train(train_path)\n",
        "\n",
        "\t\twith open(evaluation_path, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
        "\t\t\tlines = f.readlines()\n",
        "\n",
        "\t\tsentence = []\n",
        "\t\tall_pred_tags = []\n",
        "\t\tprint(dataset)\n",
        "\n",
        "\t\tfor line in lines:    \n",
        "\t\t\tif line != \"\\n\":    \n",
        "\t\t\t\tline = line.strip()\n",
        "\t\t\t\tsentence.append(line)\n",
        "\t\t\telse:\n",
        "\t\t\t\t# print(emission_count)\n",
        "\t\t\t\tsentence_prediction = viterbi_forward(emission_count, transition_count, tokens, sentence)\n",
        "\t\t\t\tsentence_prediction.remove(\"START\")\n",
        "\t\t\t\tsentence_prediction.remove(\"STOP\")\n",
        "\t\t\t\t# print(sentence_prediction)\n",
        "\t\t\t\tall_pred_tags = all_pred_tags + sentence_prediction\n",
        "\t\t\t\tall_pred_tags = all_pred_tags + [\"\\n\"]\n",
        "\t\t\t\t# print(all_pred_tags)\n",
        "\t\t\t\tsentence = []\n",
        "\t\t# print(\"length of lines:\", len(lines),\"lenth of all tags:\", len(all_pred_tags))\n",
        "\t\tassert len(lines) == len(all_pred_tags)\n",
        "\t\tprint(\"All words have a tag. Proceeding..\")\n",
        "\n",
        "\t\tif arg.test:\n",
        "\t\t\toutput_path = root_dir + \"{}/test.p4.out\".format(dataset + \"-test\")\n",
        "\t\telse:\n",
        "\t\t\toutput_path = root_dir + \"{}/dev.p4.out\".format(dataset)\n",
        "\t\t# print(all_pred_tags)\n",
        "\t\twrite_output(output_path, lines, all_pred_tags)\n",
        "\t\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3360a53796541dd2977b4996484fb33c50c413f15d574747a99ea8c321d96daf"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
