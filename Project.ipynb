{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Libraries\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset ES:\n",
      "Training..\n",
      "Evaluating..\n",
      "Writing output..\n",
      "Output successfully written!\n",
      "Dataset ES done.\n",
      "For dataset RU:\n",
      "Training..\n",
      "Evaluating..\n",
      "Writing output..\n",
      "Output successfully written!\n",
      "Dataset RU done.\n",
      "Done for all datasets!!\n"
     ]
    }
   ],
   "source": [
    "#Part 1\n",
    "def train(filepath):\n",
    "\tprint(\"Training..\")\n",
    "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\n",
    "\t# Set of all unique tokens in file\n",
    "\ttokens = []\n",
    "\t# Nested dictionary to keep track of emission count\n",
    "\t# {tag: {token: count} }\n",
    "\temission_count = {} \n",
    "\n",
    "\t# Iterate through file to update tokens and emission_count\n",
    "\tfor line in lines:\n",
    "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
    "\t\tif len(line_split) == 2:\n",
    "\t\t\ttoken = line_split[0]\n",
    "\t\t\ttag = line_split[1]\n",
    "\n",
    "\t\t\tif token not in tokens:\n",
    "\t\t\t\ttokens.append(token)\n",
    "\n",
    "\t\t\tif tag not in emission_count:\n",
    "\t\t\t\tnested_tag_dict = {}\n",
    "\t\t\telse:\n",
    "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
    "\t\t\tif token not in nested_tag_dict:\n",
    "\t\t\t\tnested_tag_dict[token] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tnested_tag_dict[token] += 1\n",
    "\t\t\temission_count[tag] = nested_tag_dict\n",
    "\n",
    "\treturn tokens, emission_count\n",
    "\n",
    "\n",
    "def est_emission_param(emission_count, token, tag):\n",
    "\ttag_dict = emission_count[tag]\n",
    "\n",
    "\ta = tag_dict.get(token, 0)\t# Returns 0 if none\n",
    "\tb = sum(tag_dict.values())\n",
    "\n",
    "\treturn a / b\n",
    "\n",
    "\n",
    "def est_emission_param(emission_count, token, tag, k=1):\n",
    "\ttag_dict = emission_count[tag]\n",
    "\n",
    "\tif token != \"#UNK#\":\n",
    "\t\ta = tag_dict.get(token, 0)\n",
    "\telse:\n",
    "\t\ta = k \n",
    "\tb = sum(tag_dict.values()) + k\n",
    "\n",
    "\treturn a / b\n",
    "\n",
    "\n",
    "def get_sentence_tag(sentence, tokens, emission_count, k=1):\n",
    "\tpred_tags = []\n",
    "\n",
    "\tfor word in sentence:\n",
    "\t\tpred_tag = \"\"\n",
    "\t\tmax_emission = float('-inf')\n",
    "\n",
    "\t\tfor tag in emission_count:\n",
    "\t\t\tif word not in tokens:\n",
    "\t\t\t\tword = \"#UNK#\"\n",
    "\n",
    "\t\t\tif word in emission_count[tag] or word == \"#UNK#\":\n",
    "\t\t\t\temission = est_emission_param(emission_count, word, tag, k)\n",
    "\t\t\t\tif emission > max_emission:\n",
    "\t\t\t\t\tpred_tag = tag \n",
    "\t\t\t\t\tmax_emission = emission\n",
    "\n",
    "\t\tpred_tags.append(pred_tag)\n",
    "\n",
    "\treturn pred_tags\n",
    "\n",
    "\n",
    "def evaluate(filepath, tokens, emission_count, k=1):\n",
    "\tprint(\"Evaluating..\")\n",
    "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\n",
    "\tall_pred_tags = []\n",
    "\n",
    "\tsentence = []\n",
    "\tfor line in lines:\n",
    "\t\tif line != \"\\n\":\n",
    "\t\t\tsentence.append(line.strip())\n",
    "\t\telse:\n",
    "\t\t\tpred_tags = get_sentence_tag(sentence, tokens, emission_count, k)\n",
    "\t\t\tall_pred_tags += pred_tags + [\"\\n\"]\n",
    "\n",
    "\t\t\tsentence = []\n",
    "\n",
    "\treturn lines, all_pred_tags\n",
    "\n",
    "\n",
    "def write_output(filepath, lines, all_pred_tags):\n",
    "\tprint(\"Writing output..\")\n",
    "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
    "\t\tfor i in range(len(lines)):\n",
    "\t\t\tword = lines[i].strip()\n",
    "\n",
    "\t\t\tif word != \"\\n\":\n",
    "\t\t\t\ttag = all_pred_tags[i]\n",
    "\n",
    "\t\t\t\tif tag != \"\\n\":\n",
    "\t\t\t\t\tf.write(word + \" \" + tag)\n",
    "\t\t\t\tf.write(\"\\n\")\n",
    "\n",
    "\tprint(\"Output successfully written!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\troot_dir = \"./\"\n",
    "\tdatasets = [\"ES\", \"RU\"]\n",
    "\n",
    "\tfor dataset in datasets:\n",
    "\t\tprint(\"For dataset {}:\".format(dataset))\n",
    "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
    "\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
    "\n",
    "\t\t# Train\n",
    "\t\ttokens, emission_count = train(train_path)\n",
    "\n",
    "\t\t# Estimate emission parameters using MLE\n",
    "\t\t\"\"\"\n",
    "\t\tfor tag in emission_count:\n",
    "\t\t\tfor token in tokens:\n",
    "\t\t\t\t#emission = est_emission_param(emission_count, token, tag)\t\t# Without tackling special word token\n",
    "\t\t\t\temission = est_emission_param(emission_count, token, tag, k=1)\t# With special word token\n",
    "\t\t\t\tif emission != 0:\n",
    "\t\t\t\t\tprint(\"Emission parameters for {x} given {y}: {emission}\".format(x=token, y=tag, emission=emission))\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\t# Evaluate\n",
    "\t\tlines, all_pred_tags = evaluate(evaluation_path, tokens, emission_count, k=1)\n",
    "\n",
    "\t\t# Write output file\n",
    "\t\toutput_path = root_dir + \"{}/dev.p1.out\".format(dataset)\n",
    "\t\twrite_output(output_path, lines, all_pred_tags)\n",
    "\n",
    "\t\tprint(\"Dataset {} done.\".format(dataset))\n",
    "\n",
    "\tprint(\"Done for all datasets!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset ES:\n",
      "{'START': {'O': 1918, 'B-negative': 27, 'B-positive': 110, 'B-neutral': 10}, 'O': {'O': 27939, 'B-positive': 1162, 'STOP': 2050, 'B-negative': 402, 'B-neutral': 74}, 'B-positive': {'O': 1100, 'I-positive': 162, 'B-positive': 2, 'STOP': 9, 'B-neutral': 1}, 'B-negative': {'O': 347, 'I-negative': 78, 'STOP': 4}, 'I-negative': {'I-negative': 151, 'O': 78}, 'I-positive': {'O': 160, 'I-positive': 238, 'STOP': 2}, 'B-neutral': {'O': 69, 'I-neutral': 16}, 'I-neutral': {'O': 16, 'I-neutral': 28}}\n",
      "Training..\n",
      "ES\n",
      "{'START': {'O': 1918, 'B-negative': 27, 'B-positive': 110, 'B-neutral': 10}, 'O': {'O': 27939, 'B-positive': 1162, 'STOP': 2050, 'B-negative': 402, 'B-neutral': 74}, 'B-positive': {'O': 1100, 'I-positive': 162, 'B-positive': 2, 'STOP': 9, 'B-neutral': 1}, 'B-negative': {'O': 347, 'I-negative': 78, 'STOP': 4}, 'I-negative': {'I-negative': 151, 'O': 78}, 'I-positive': {'O': 160, 'I-positive': 238, 'STOP': 2}, 'B-neutral': {'O': 69, 'I-neutral': 16}, 'I-neutral': {'O': 16, 'I-neutral': 28}}\n",
      "['START', 'O', 'B-positive', 'B-negative', 'I-negative', 'I-positive', 'B-neutral', 'I-neutral']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'I-neutral'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c5/zncr7wnn1_gb_h3dmz74blf00000gn/T/ipykernel_14410/4251947797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    246\u001b[0m                                 \u001b[0;31m# print(emission_count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                                 \u001b[0msentence_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memission_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m                                 \u001b[0msentence_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"START\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                                 \u001b[0msentence_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STOP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/c5/zncr7wnn1_gb_h3dmz74blf00000gn/T/ipykernel_14410/4251947797.py\u001b[0m in \u001b[0;36mviterbi_forward\u001b[0;34m(emissions, transitions, words, labels)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                                 \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0memission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                                 \u001b[0mfindmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'I-neutral'"
     ]
    }
   ],
   "source": [
    "#Part 2\n",
    "def train(filepath):\n",
    "\tprint(\"Training..\")\n",
    "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\t\n",
    "\tstart = \"START\"\n",
    "\tstop = \"STOP\"\n",
    "\n",
    "\t# Set of all unique tokens in file\n",
    "\ttokens = []\n",
    "\t# Nested dictionary to keep track of emission count\n",
    "\t# {tag: {token: count} }\n",
    "\temission_count = {} \n",
    "\n",
    "\t# Iterate through file to update tokens and emission_count\n",
    "\tfor line in lines:\n",
    "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
    "\t\tif len(line_split) == 2:\n",
    "\t\t\ttoken = line_split[0]\n",
    "\t\t\ttag = line_split[1]\n",
    "\n",
    "\t\t\tif token not in tokens:\n",
    "\t\t\t\ttokens.append(token)\n",
    "\n",
    "\t\t\tif tag not in emission_count:\n",
    "\t\t\t\tnested_tag_dict = {}\n",
    "\t\t\telse:\n",
    "\t\t\t\tnested_tag_dict = emission_count[tag]\n",
    "\t\t\tif token not in nested_tag_dict:\n",
    "\t\t\t\tnested_tag_dict[token] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tnested_tag_dict[token] += 1\n",
    "\t\t\temission_count[tag] = nested_tag_dict\n",
    "\t\t\t\n",
    "\treturn tokens, emission_count\n",
    "\n",
    "\n",
    "def est_emission_param(emission_count, token, tag, k=1):\n",
    "\ttag_dict = emission_count[tag]\n",
    "\n",
    "\tif token != \"#UNK#\":\n",
    "\t\ta = tag_dict.get(token, 0)\n",
    "\telse:\n",
    "\t\ta = k \n",
    "\tb = sum(tag_dict.values()) + k\n",
    "\n",
    "\treturn a / b\n",
    "\n",
    "\n",
    "def transition(filepath):\n",
    "\twith open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "\t\tlines = f.readlines()\n",
    "\t\t\n",
    "\tstart = \"START\"\n",
    "\tstop = \"STOP\"\n",
    "\n",
    "\t# Set of all unique tokens in file\n",
    "\ttokens = []\n",
    "\t# Nested dictionary to keep track of emission count\n",
    "\t# {tag: {token: count} }\n",
    "\tu = start\n",
    "\ttransition_count = {} \n",
    "\n",
    "\t# Iterate through file to update tokens and transition_count\n",
    "\tfor line in lines:\n",
    "\t\tline_split = line.strip().rsplit(\" \", 1)\n",
    "\t\t\n",
    "\t\t#Case 1\n",
    "\t\t\n",
    "\t\tif len(line_split) == 2:\n",
    "\t\t\ttoken = line_split[0]\n",
    "\t\t\tv = line_split[1]\n",
    "\n",
    "\t\t\tif u not in transition_count:\n",
    "\t\t\t\tu_dict = {}\n",
    "\t\t\telse:\n",
    "\t\t\t\tu_dict = transition_count[u]\n",
    "\n",
    "\t\t\tif v in u_dict:\n",
    "\t\t\t\tu_dict[v] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tu_dict[v] = 1\n",
    "\n",
    "\t\t\ttransition_count[u] = u_dict\n",
    "\t\t\tu = v\n",
    "\n",
    "\t\t#Case 2\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\tu_dict = transition_count[u]\n",
    "\t\t\tv = stop\n",
    "\n",
    "\t\t\tif v in u_dict:\n",
    "\t\t\t\tu_dict[v] += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tu_dict[v] = 1\n",
    "\n",
    "\t\t\ttransition_count[u] = u_dict\n",
    "\t\t\tu = start\n",
    "\tprint(transition_count)\n",
    "\treturn transition_count\n",
    "\n",
    "def transition_param(transition_count, u, v):\n",
    "\t\n",
    "\n",
    "\tif u not in transition_count:\n",
    "\t\ta = 0\n",
    "\telse:\n",
    "\t\tu_dict = transition_count[u]\n",
    "\t\ta = u_dict.get(v,0)\n",
    "\t\tb = sum(u_dict.values())\n",
    "\n",
    "\treturn a / b\n",
    "\n",
    "def viterbi_forward(emissions, transitions, words, labels):\n",
    "\tn = len(labels)\n",
    "\tsmallest = -9999\n",
    "\t\n",
    "\tstates = list(transitions.keys())\n",
    "\tprint(states)\n",
    "\tstates.remove(\"START\")\n",
    "\n",
    "\t# initialize score dict\n",
    "\tscores = {}\n",
    "\n",
    "\tscores[0] = {}\n",
    "\n",
    "\tfor v in states:\n",
    "\t\ttransition_fraction = transition_param(transitions, \"START\", v)\n",
    "\t\tif transition_fraction != 0:\n",
    "\t\t\ttrans = math.log(transition_fraction)\n",
    "\t\telse:\n",
    "\t\t\ttrans = smallest\n",
    "\n",
    "\t\tif labels[0] not in words:\n",
    "\t\t\ttoken = \"#UNK#\"\n",
    "\t\telse:\n",
    "\t\t\ttoken = labels[0]\n",
    "\n",
    "\t\t# Emission Probability\n",
    "\t\tif ((token in emissions[v]) or (token == \"#UNK#\")): \n",
    "\t\t\temmision_fraction = est_emission_param(emissions, token, v)\n",
    "\t\t\temission = math.log(emmision_fraction)\n",
    "\t\telse:\n",
    "\t\t\temission = smallest\n",
    "\n",
    "\t\tstart = trans + emission\n",
    "\t\tscores[0][v] = (\"START\", start)\n",
    "        \n",
    "\t\t# State 1 to n\n",
    "\t\tfor i in range(1, n):\n",
    "\n",
    "\t\t\tscores[i] = {}\n",
    "\t\t\tfor v in states:\n",
    "\t\t\t\tfindmax = []\n",
    "\n",
    "\t\t\t\tfor u in states:\n",
    "                \t# Transition Probability\n",
    "\t\t\t\t\ttransition_fraction = transition_param(transitions, u, v)\n",
    "\n",
    "\t\t\t\t\tif transition_fraction != 0:\n",
    "\t\t\t\t\t\ttrans = math.log(transition_fraction)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttrans = smallest\n",
    "                # if the word does not exist, assign special token\n",
    "\t\t\t\tif labels[i] not in words:\n",
    "\t\t\t\t\ttoken = \"#UNK#\"\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttoken = labels[i]\n",
    "\n",
    "                # Emission Probability\n",
    "\t\t\t\tif ((token in emissions[v]) or token == \"#UNK#\"):\n",
    "\t\t\t\t\temission_fraction = est_emission_param(emissions, token, v)\n",
    "\t\t\t\t\temission = math.log(emission_fraction)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\temission = smallest\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t\tcurrent = scores[i-1][u][1] + trans + emission\n",
    "\t\t\t\tfindmax.append(current)\n",
    "    \n",
    "\n",
    "            # ARGMAX\n",
    "\t\t\tans = max(findmax)\n",
    "\t\t\tstate_ans = states[findmax.index(ans)]\n",
    "\t\t\tscores[i][v] = (state_ans, ans)\n",
    "\n",
    "\t# STATE N to Stop State\n",
    "\tscores[n] = {}\n",
    "\tstopmax = []\n",
    "\tfor u in states:\n",
    "        # Transition Probability\n",
    "\t\ttransition_fraction = transition_param(transitions, u, \"STOP\")\n",
    "\t\tif transition_fraction != 0:\n",
    "\t\t\ttransition = math.log(transition_fraction)\n",
    "\t\telse:\n",
    "\t\t\ttransition = smallest\n",
    "        \n",
    "\t\tstopscore = scores[n-1][u][1] + trans\n",
    "\t\tstopmax.append(stopscore)\n",
    "    \n",
    "    # ARGMAX\n",
    "\tstop = max(stopmax)\n",
    "\tstate_ans = states[stopmax.index(stop)]\n",
    "\tscores[n] = (state_ans, stop)\n",
    "\n",
    "\t# Backtracking path\n",
    "\tpath = [\"STOP\"]\n",
    "    # scores[n] = ('O', -308.32462005568965)\n",
    "\tlast = scores[n][0]\n",
    "\tpath.insert(0, last)\n",
    "\n",
    "\tfor k in range(n-1, -1, -1):\n",
    "\t\t\n",
    "\t\tlast = scores[k][last][0]\n",
    "\t\tpath.insert(0, last)\n",
    "\treturn path\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\troot_dir = \"./\"\n",
    "\tdatasets = [\"ES\", \"RU\"]\n",
    "\n",
    "\tfor dataset in datasets:\n",
    "\t\tprint(\"For dataset {}:\".format(dataset))\n",
    "\t\ttrain_path = root_dir + \"{}/train\".format(dataset)\n",
    "\t\tevaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
    "\n",
    "\t\t# Train\n",
    "\t\ttransition_count = transition(train_path)\n",
    "\t\ttokens, emission_count = train(train_path)\n",
    "\n",
    "\t\twith open(evaluation_path, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
    "\t\t\tlines = f.readlines()\n",
    "\n",
    "\t\tlabels = []\n",
    "\t\tall_prediction = []\n",
    "\t\tprint(dataset)\n",
    "\n",
    "\t\tfor line in lines:    \n",
    "\t\t\tif line != \"\\n\":    \n",
    "\t\t\t\tline = line.strip()\n",
    "\t\t\t\tlabels.append(line)\n",
    "\t\t\telse:\n",
    "\t\t\t\t# print(emission_count)\n",
    "\t\t\t\tprint(transition_count)\n",
    "\t\t\t\tsentence_prediction = viterbi_forward(emission_count, transition_count, tokens, labels)\n",
    "\t\t\t\tsentence_prediction.remove(\"START\")\n",
    "\t\t\t\tsentence_prediction.remove(\"STOP\")\n",
    "\t\t\t\tall_prediction = all_prediction + sentence_prediction\n",
    "\t\t\t\tall_prediction = all_prediction + [\"\\n\"]\n",
    "\t\t\t\tlabels = []\n",
    "        \n",
    "\t\tassert len(lines) == len(all_prediction)\n",
    "\t\tprint(\"All words have a tag. Proceeding..\")\n",
    "\n",
    "        # create output file\n",
    "\t\twith open(root_dir + \"{folder}/dev.p2.out\".format(folder = dataset), \"w\", encoding=\"utf8\", errors='ignore') as g:\n",
    "\t\t\tfor j in range(len(lines)):\n",
    "\t\t\t\tword = lines[j].strip()\n",
    "\t\t\t\tif word != \"\\n\":\n",
    "\t\t\t\t\ttag = all_prediction[j]\n",
    "\t\t\t\t\tif(tag != \"\\n\"):\n",
    "\t\t\t\t\t\tg.write(word + \" \" + tag)\n",
    "\t\t\t\t\t\tg.write(\"\\n\")\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tg.write(\"\\n\")\n",
    "\tprint(\"done\")\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3360a53796541dd2977b4996484fb33c50c413f15d574747a99ea8c321d96daf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
