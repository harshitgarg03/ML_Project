{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES\n",
      "length of lines: 5560 lenth of all tags: 5560\n",
      "All words have a tag. Proceeding..\n",
      "Writing output..\n",
      "Output successfully written!\n",
      "RU\n",
      "length of lines: 8053 lenth of all tags: 8053\n",
      "All words have a tag. Proceeding..\n",
      "Writing output..\n",
      "Output successfully written!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_dir = \"./\"\n",
    "\n",
    "def train(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    tokens3 = set()\n",
    "    \n",
    "    emission_count = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        line_split = line.strip().rsplit(\" \", 1)  \n",
    "        \n",
    "        if len(line_split) == 2:\n",
    "            token3 = line_split[0]\n",
    "            tag = line_split[1]        \n",
    "            tokens3.add(token3)\n",
    "    \n",
    "            \n",
    "            if tag in emission_count:\n",
    "               \n",
    "                nested_tag_dict = emission_count[tag]\n",
    "            else:\n",
    "                \n",
    "                nested_tag_dict = {}\n",
    "                \n",
    "            if token3 in nested_tag_dict:\n",
    "                nested_tag_dict[token3] = nested_tag_dict[token3] + 1\n",
    "            else:\n",
    "                nested_tag_dict[token3] = 1\n",
    "            \n",
    "            emission_count[tag] = nested_tag_dict\n",
    "    \n",
    "    return tokens3, emission_count\n",
    "\n",
    "def est_emission_param(emission_count, token, tag, k = 1):\n",
    "    tag_dict = emission_count[tag]\n",
    "    \n",
    "    b = sum(tag_dict.values()) + k\n",
    "    \n",
    "    if token != \"#UNK#\":\n",
    "        a = tag_dict[token]\n",
    "    else: \n",
    "        a = k\n",
    "    \n",
    "    return a / b\n",
    "\n",
    "def tag_producer(emission_count, labels, tokens3):\n",
    "    tag_output = []\n",
    "    \n",
    "    for i in labels:\n",
    "        predicted_state = \"\"\n",
    "        highest_prob = -9999999.0\n",
    "        for tag in emission_count:\n",
    "            if i not in tokens3:\n",
    "                i = \"#UNK#\"\n",
    "                \n",
    "            if ((i in emission_count[tag]) or (i == \"#UNK#\")):\n",
    "                emission_prob = est_emission_param(emission_count, i, tag, 1)\n",
    "\n",
    "                if emission_prob > highest_prob:\n",
    "                    highest_prob = emission_prob\n",
    "                    predicted_state = tag\n",
    "                    \n",
    "        tag_output.append(predicted_state)\n",
    "    return tag_output\n",
    "        \n",
    "\n",
    "def transition(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf8\", errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    start = \"START\"\n",
    "    stop = \"STOP\"\n",
    "    \n",
    "    u = start\n",
    "    \n",
    "    transition_count = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        line_split = line.strip().rsplit(\" \", 1)  \n",
    "        \n",
    "        # case 1\n",
    "        if len(line_split) == 2:\n",
    "            token3 = line_split[0]\n",
    "            v = line_split[1]\n",
    "            if u not in transition_count:\n",
    "                u_dict = {}\n",
    "            else:\n",
    "                u_dict = transition_count[u]\n",
    "            \n",
    "            if v in u_dict:\n",
    "                u_dict[v] += 1\n",
    "            else:\n",
    "                u_dict[v] = 1\n",
    "            transition_count[u] = u_dict\n",
    "            u = v\n",
    "            \n",
    "        if len(line_split) != 2:\n",
    "            u_dict = transition_count[u]\n",
    "            v = stop\n",
    "            if v in u_dict:\n",
    "                u_dict[v] += 1\n",
    "            else:\n",
    "                u_dict[v] = 1\n",
    "            transition_count[u] = u_dict\n",
    "            \n",
    "            u = start\n",
    "            \n",
    "            \n",
    "    return transition_count\n",
    "\n",
    "def transition_para(transition_count, u, v):\n",
    "    if u not in transition_count:\n",
    "        a = 0\n",
    "        \n",
    "    else:\n",
    "        u_dict = transition_count[u]\n",
    "    \n",
    "        a = u_dict.get(v, 0)\n",
    "    \n",
    "        b = sum(u_dict.values())\n",
    "        \n",
    "    \n",
    "    return a/b\n",
    "\n",
    "def viterbi_forward(N, emissions, transitions, words, labels):\n",
    "    n = len(labels)\n",
    "    smallest = -9999999\n",
    "\n",
    "    states = list(transitions.keys())\n",
    "    states.remove(\"START\")\n",
    "\n",
    "    scores = {}\n",
    "    scores[0] = {}\n",
    "\n",
    "    for v in states:\n",
    "        # Transition Probability\n",
    "        transition_fraction = transition_para(transitions, \"START\", v)\n",
    "        if transition_fraction != 0:\n",
    "            trans = math.log(transition_fraction)\n",
    "        else:\n",
    "            trans = smallest\n",
    "        \n",
    "        if labels[0] not in words:\n",
    "            token3 = \"#UNK#\"\n",
    "        else:\n",
    "            token3 = labels[0]\n",
    "\n",
    "        # Emission Probability\n",
    "        if ((token3 in emissions[v]) or (token3 == \"#UNK#\")): \n",
    "            emmision_fraction = est_emission_param(emissions, token3, v)\n",
    "            emission = math.log(emmision_fraction)\n",
    "        else:\n",
    "            emission = smallest\n",
    "        \n",
    "        start = trans + emission\n",
    "        scores[0][v] = (\"START\", start)\n",
    "\n",
    "    copyscores = copy.deepcopy(scores)\n",
    "    \n",
    "    # State 1 to n\n",
    "    for i in range(1, n):\n",
    "        scores[i] = {}\n",
    "        copyscores[i] = {}\n",
    "        for v in states:\n",
    "            findmax = []\n",
    "            for u in states:\n",
    "                # Transition Probability\n",
    "                transition_fraction = transition_para(transitions, u, v)\n",
    "                if transition_fraction != 0:\n",
    "                    trans = math.log(transition_fraction)\n",
    "                else:\n",
    "                    trans = smallest\n",
    "                if labels[i] not in words:\n",
    "                    v_v = \"#UNK#\"\n",
    "                else:\n",
    "                    v_v = labels[i]\n",
    "\n",
    "                # Emission Probability\n",
    "                if ((v_v in emissions[v]) or (v_v == \"#UNK#\")): \n",
    "                    emmision_fraction = est_emission_param(emissions, v_v, v)\n",
    "                    emission = math.log(emmision_fraction)\n",
    "                else:\n",
    "                    emission = smallest\n",
    "              \n",
    "                if i == 1 :\n",
    "                  currentscore = scores[i-1][u][1] + trans + emission\n",
    "                  findmax.append(currentscore)\n",
    "                else:\n",
    "                  currentscores = [[scores[i-1][u][m][1] for m in range(N)][j] + trans + emission for j in range(N)] # currentscores = [bestscore, 2nd bestscore, 3rd bestscore]\n",
    "                  for score in currentscores:\n",
    "                    findmax.append(score)\n",
    "            ans = [] \n",
    "            state_ans = []\n",
    "            copyfindmax = copy.deepcopy(findmax)\n",
    "            for m in range(N):\n",
    "                ans.append(max(copyfindmax))\n",
    "                state_ans.append(states[findmax.index(ans[m]) // N])\n",
    "                copyfindmax[findmax.index(ans[m])] = -999999999.999\n",
    "            scores[i][v] = tuple((state_ans[m], ans[m]) for m in range(N))\n",
    "            \n",
    "\n",
    "    # STOP STATE\n",
    "    scores[n] = {}\n",
    "    copyscores[n] = {}\n",
    "    stopmax = []\n",
    "\n",
    "    for u in states:\n",
    "        # Transition Probability\n",
    "        transition_fraction = transition_para(transitions, u, \"STOP\")\n",
    "        if transition_fraction != 0:\n",
    "            trans = math.log(transition_fraction)\n",
    "        else:\n",
    "            trans = smallest\n",
    "\n",
    "        if(type(scores[n-1][u][0])==tuple):\n",
    "            stopscore = [[scores[n-1][u][m][1] for m in range(N)][j] + trans + emission for j in range(N)]\n",
    "        else:\n",
    "            t=scores[n-1][u]\n",
    "            stopscore = [t[1]+ trans + emission]    \n",
    "            \n",
    "        for score in stopscore:\n",
    "            stopmax.append(score)\n",
    "            \n",
    "\n",
    "    stop = []\n",
    "    state_ans = []\n",
    "    copystopmax = copy.deepcopy(stopmax)\n",
    "    for i in range(N):\n",
    "        stop.append(max(copystopmax))\n",
    "        state_ans.append(states[stopmax.index(stop[i]) // N])\n",
    "        copystopmax[stopmax.index(stop[i])] = -999999999.999\n",
    "    scores[n][u] = tuple((state_ans[m], stop[m]) for m in range(N))\n",
    "    \n",
    "      \n",
    "    N_bestPaths = []\n",
    "    lasts = [] \n",
    "    for i in range(N):\n",
    "      path = [\"STOP\"]\n",
    "      last = list(scores[n].values())[0][i][0]\n",
    "      lasts.append(last)\n",
    "      path.insert(0, last)\n",
    "      N_bestPaths.append(path)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for k in range(n-1, -1, -1):\n",
    "            if k == 0:\n",
    "                last = scores[k][N_bestPaths[i][0]][0] \n",
    "            else:\n",
    "                last = scores[k][N_bestPaths[i][0]][0][0]\n",
    "            N_bestPaths[i].insert(0, last)\n",
    "    \n",
    "    \n",
    "    return N_bestPaths[N-1]\n",
    "\n",
    "def write_output(filepath, lines, all_pred_tags):\n",
    "\tprint(\"Writing output..\")\n",
    "\twith open(filepath, \"w\", encoding=\"utf8\") as f:\n",
    "\t\tfor j in range(len(lines)):\n",
    "\t\t\tword = lines[j].strip()\n",
    "\t\t\tif word != \"\\n\":\n",
    "\t\t\t\t# print(word)\n",
    "\t\t\t\ttag = all_pred_tags[j]\n",
    "\t\t\t\t# print(tag)\n",
    "\t\t\t\tif(tag != \"\\n\"):\n",
    "\t\t\t\t\tf.write(word + \" \" + tag)\n",
    "\t\t\t\t\tf.write(\"\\n\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tf.write(\"\\n\")\n",
    "\n",
    "\tprint(\"Output successfully written!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = [\"ES\", \"RU\"]\n",
    "\n",
    "    for dataset in datasets:\n",
    "\n",
    "        train_path = root_dir + \"{}/train\".format(dataset)\n",
    "        evaluation_path = root_dir + \"{}/dev.in\".format(dataset)\n",
    "        \n",
    "        # training\n",
    "        transition_count = transition(train_path)\n",
    "\n",
    "        tokens3, emission_count = train(train_path)\n",
    "        \n",
    "        # evaluation\n",
    "        with open(evaluation_path, \"r\", encoding='utf8') as f:\n",
    "            lines = f.readlines()\n",
    "        labels = []\n",
    "        \n",
    "        all_pred_tags = []\n",
    "        print(dataset)\n",
    "        N = 5\n",
    "        for line in lines:    \n",
    "            if line != \"\\n\":    \n",
    "                line = line.strip()\n",
    "                labels.append(line)\n",
    "            else:\n",
    "                # print(emission_count)\n",
    "                sentence_prediction = viterbi_forward(N, emission_count, transition_count, tokens3, labels)\n",
    "                sentence_prediction.remove(\"START\")\n",
    "                sentence_prediction.remove(\"STOP\")\n",
    "                # print(sentence_prediction)\n",
    "                all_pred_tags = all_pred_tags + sentence_prediction\n",
    "                all_pred_tags = all_pred_tags + [\"\\n\"]\n",
    "                # print(all_pred_tags)\n",
    "                labels = []\n",
    "        # print(\"length of lines:\", len(lines),\"lenth of all tags:\", len(all_pred_tags))\n",
    "        assert len(lines) == len(all_pred_tags)\n",
    "        print(\"All words have a tag. Proceeding..\")\n",
    "\n",
    "        output_path = root_dir + \"{}/dev.p5.out\".format(dataset)\n",
    "        # print(all_pred_tags)\n",
    "        write_output(output_path, lines, all_pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3360a53796541dd2977b4996484fb33c50c413f15d574747a99ea8c321d96daf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
